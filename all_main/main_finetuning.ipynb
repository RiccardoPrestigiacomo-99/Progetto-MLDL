{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoPrestigiacomo-99/Progetto-MLDL/blob/main/all_main/main_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j40FSXGxD2VD"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uudv9Cj8E8OI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11f501cc-c15d-4e70-8779-5662e56aae46"
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dC-rYdjD-E3"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lf-WK3hEJCM"
      },
      "source": [
        "**Retrieving dataset CIFAR1000**<br>\n",
        "The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images per class. There are 50000 training images and 10000 test images. There are 500 training images and 100 testing images per class.\n",
        "The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
        "Here is an example of classes in the CIFAR-100:<br>\n",
        "**Superclass**\t\n",
        "- aquatic mammals\t\n",
        "\n",
        "**Classes**\n",
        "- beaver, dolphin, otter, seal, whale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n1do9ln3OVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b79f9ce1-03a2-4a8a-8ae3-07f25ef289ad"
      },
      "source": [
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/\n",
        "!rm -r $DATA_DIR\n",
        "!mkdir \"DATA\"\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone https://github.com/RiccardoPrestigiacomo-99/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 22 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), done.\n",
            "rm: cannot remove 'Cifar100/Theoretical-Sources': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f96c29-3882-499b-ebda-1ce15c9aa464"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-05 09:23:57--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  16.9MB/s    in 11s     \n",
            "\n",
            "2021-06-05 09:24:09 (15.1 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XOn3bHMEBzX"
      },
      "source": [
        "**Set arguments** - \n",
        "src: iCaRL section 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-pqSNg4_Ris",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cdfaf78-f72a-430f-9d4b-d22619f1f736"
      },
      "source": [
        "from Cifar100 import utils\n",
        "dictHyperparams = utils.getHyperparams()\n",
        "print(dictHyperparams)\n",
        "\n",
        "DEVICE = dictHyperparams[\"DEVICE\"] # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = dictHyperparams[\"NUM_CLASSES\"] \n",
        "\n",
        "BATCH_SIZE = dictHyperparams[\"BATCH_SIZE\"]     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = dictHyperparams[\"LR\"]          # The initial Learning Rate\n",
        "MOMENTUM = dictHyperparams[\"MOMENTUM\"]       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = dictHyperparams[\"WEIGHT_DECAY\"] # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = dictHyperparams[\"NUM_EPOCHS\"]     # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = dictHyperparams[\"GAMMA\"]         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = dictHyperparams[\"LOG_FREQUENCY\"]\n",
        "MILESTONES = dictHyperparams[\"MILESTONES\"]\n",
        "RANDOM_SEED = dictHyperparams[\"SEED\"]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LR': 2, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 1e-05, 'NUM_EPOCHS': 70, 'MILESTONES': [49, 63], 'BATCH_SIZE': 128, 'DEVICE': 'cuda', 'GAMMA': 0.2, 'SEED': 66, 'LOG_FREQUENCY': 10, 'NUM_CLASSES': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oJ5m4V-ERDh"
      },
      "source": [
        "**Define data preprocessing**<br>\n",
        "This transformations are applied to each images when they're loaded into the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_jHycn_1kk"
      },
      "source": [
        "train_transform, eval_transform = utils.getTransformations()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7EVuDrcJj2N"
      },
      "source": [
        "**Prepare dataset**<br>\n",
        "Loading of the train and test split as it comes with CIFAR100. <br>\n",
        "The trainset consists in 50k images, while the test set len is 10k images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJKwvGljJj2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543ba9f4-05fb-4510-f149-1768e8b7ea3a"
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# check if datasets have been correctly loaded\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckn3H69iJj2X"
      },
      "source": [
        "from Cifar100.reverse_index import ReverseIndex\n",
        "\n",
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYLmmQn7JOLc"
      },
      "source": [
        "**Build dataset splits and reverse index**<br>\n",
        "Here the train dataset is split into train and validation set, following the proportion XX/YY.<br>\n",
        "Furthermore train, test and validation sets are splitted into 10 groups containing 10 classes each (the split is coherent among the different sets).<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpcJvhxhJOLc"
      },
      "source": [
        "# performing the train/val split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "\n",
        "# performing the test split (coherent with train/val)\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fov9YAFTlj"
      },
      "source": [
        "**Prepare dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "77b9ec6b-4d29-47cd-f0ad-072458c4c38e"
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)\n",
        "\n",
        "# [ DEBUG ]\n",
        "# test to check classes in different dataset\n",
        "# (coherent split)\n",
        "# RESULT: OK\n",
        "\"\"\"dict_train={}\n",
        "for img_train in train_subsets[0]:\n",
        "  if img_train[1] not in dict_train:\n",
        "    dict_train[img_train[1]]=1\n",
        "  else:\n",
        "    dict_train[img_train[1]]+=1\n",
        "dict_val={}\n",
        "for img_val in val_subsets[0]:\n",
        "  if img_val[1] not in dict_val:\n",
        "    dict_val[img_val[1]]=1\n",
        "  else:\n",
        "    dict_val[img_val[1]]+=1\n",
        "dict_test={}\n",
        "for img_test in test_subsets[0]:\n",
        "  if img_test[1] not in dict_test:\n",
        "    dict_test[img_test[1]]=1\n",
        "  else:\n",
        "    dict_test[img_test[1]]+=1\n",
        "\n",
        "print(sorted(dict_test.keys()))\n",
        "print(sorted(dict_test.keys()))\n",
        "print(sorted(dict_test.keys()))\"\"\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dict_train={}\\nfor img_train in train_subsets[0]:\\n  if img_train[1] not in dict_train:\\n    dict_train[img_train[1]]=1\\n  else:\\n    dict_train[img_train[1]]+=1\\ndict_val={}\\nfor img_val in val_subsets[0]:\\n  if img_val[1] not in dict_val:\\n    dict_val[img_val[1]]=1\\n  else:\\n    dict_val[img_val[1]]+=1\\ndict_test={}\\nfor img_test in test_subsets[0]:\\n  if img_test[1] not in dict_test:\\n    dict_test[img_test[1]]=1\\n  else:\\n    dict_test[img_test[1]]+=1\\n\\nprint(sorted(dict_test.keys()))\\nprint(sorted(dict_test.keys()))\\nprint(sorted(dict_test.keys()))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d19CnhIUg0q"
      },
      "source": [
        "**Utility functions to use our customized resnet model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ff9pwV10b0v"
      },
      "source": [
        "from Cifar100.resnet import resnet32\n",
        "\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getResNet32():\n",
        "    net = resnet32()\n",
        "    # net.fc = nn.Linear(net.fc.in_features, output_size) # embedded in the class\n",
        "\n",
        "    criterion = utils.getLossCriterion()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = utils.getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getNet():\n",
        "    return getResNet32()\n",
        "\n",
        "def getSchedulerOptimizer(net):\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = utils.getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return optimizer, scheduler"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glyt2p3XTEqt"
      },
      "source": [
        "**Basic train, test and validation functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzG6w15UudAP"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, num_epochs=NUM_EPOCHS):     \n",
        "    # By default, everything is loaded to cpu\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "    \n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "            # Bring data over the device of choice\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            labels_enc = utils._one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = utils.computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            # preds = getLabels(outputs_labels_mapping, preds)\n",
        "            # print(preds)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    utils.getLossCriterion()\n",
        "\n",
        "    # confusion matrix\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "        # Bring data over the device of choice\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        #labels = outputs_labels_mapping.getNodes(labels)\n",
        "        labels_enc = utils._one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        outputs = net(images)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = utils.computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        # preds = getLabels(outputs_labels_mapping, preds)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    # Calculate Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiWapdlNNsA0"
      },
      "source": [
        "**Joint Training**<br>\n",
        "In this section joint training is perfomed.<br>\n",
        "The train of the network is split into 10 stages, one for each subset classes.\n",
        "At each step, the network is trained on the images corresponding to the current 10 classes and all the data already seen in the previous steps.\n",
        "The joint training score, evaluated in terms of accuracy on the test set, gives us an UB for the next methodologies examined in this project (iCaRL, LWF).<br>\n",
        "Operatively, what happens is a very slow training and, furthermore, we break the assumption of not needing the previous batches of data at each step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4InuBhsENpV9"
      },
      "source": [
        "####\n",
        "## Joint training\n",
        "####\n",
        "# Joins 2+ subsets into a new Subset (joint training)\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "def jointTraining(getNet, addOutputs, train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getNet()\n",
        "\n",
        "    train_set = None\n",
        "    test_set = None\n",
        "    first_pass = True\n",
        "\n",
        "    current_train_num = 0\n",
        "    total_trains = len(train_subsets)\n",
        "    joint_start = time.time()\n",
        "\n",
        "    print('\\n\\nJoint-training start\\n\\n')\n",
        "    all_accuracies=[]\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "        phase_start = time.time()\n",
        "        print('\\n\\nJoint phase {}/{}\\n\\n'.format(current_train_num+1, total_trains))\n",
        "        current_train_num += 1\n",
        "\n",
        "        #num_classes_per_group = 10\n",
        "        num_classes_seen = current_train_num*10\n",
        "\n",
        "        # Builds growing train and test set. The new sets include data from previous class groups and current class group\n",
        "        if train_set is None:\n",
        "            train_set = train_subset\n",
        "        else:\n",
        "            train_set = joinSubsets(train_dataset, [train_set, train_subset])\n",
        "        if test_set is None:\n",
        "            test_set = test_subset\n",
        "        else:\n",
        "            test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "        if first_pass:\n",
        "            first_pass = False\n",
        "        else:\n",
        "            addOutputs(net, 10)\n",
        "\n",
        "        # Trains model on previous and current class groups\n",
        "        optimizer, scheduler = getSchedulerOptimizer(net)\n",
        "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen)\n",
        "\n",
        "        # Validate model on current class group\n",
        "        val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        v_acc, v_loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "        print('\\nValidation accuracy: {} - Validation loss: {}\\n'.format(v_acc, v_loss))\n",
        "\n",
        "        # Test the model on previous and current class groups\n",
        "        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "        all_accuracies.append(acc_all)\n",
        "        print('\\nTest accuracy: {}\\n'.format(acc_all))\n",
        "\n",
        "        print('\\n\\nPhase completed in {} seconds\\n\\n'.format(time.time() - phase_start))\n",
        "    \n",
        "    print('\\n\\n Joint-training finished in {} seconds'.format(time.time() - joint_start))\n",
        "    return net, all_accuracies, all_preds_cm, all_labels_cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOiPlN8ON4Gn"
      },
      "source": [
        "**Test joint training**<br>\n",
        "What we expect is a test accuracy higher of what we'll be able to achieve using iCaRL, LWF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VDecUBiHl4G"
      },
      "source": [
        "# Test Joint training\n",
        "\n",
        "net, all_accuracies, all_preds_cm, all_labels_cm = jointTraining(getNet, addOutputs, train_subsets, val_subsets, test_subsets)\n",
        "\n",
        "# output Joint training\n",
        "method = \"jointtraining\"\n",
        "print(\"metrics jointtraining for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_line.append(((id+1)*10,all_accuracies[id]))\n",
        "\n",
        "#plt.figure(figsize=(20,7))\n",
        "#accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "#ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "#plt.title(\"Single Group Sequential Accuracy\")\n",
        "#plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "utils.plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "utils.plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "utils.writeMetrics(method, RANDOM_SEED, all_accuracies, confusionMatrixData)\n",
        "\n",
        "# [DEBUG]\n",
        "#net, criterion, optimizer, scheduler = getResNet32()\n",
        "#train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "#train(net, train_dataloader, criterion, optimizer, scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pts78KY42gXj"
      },
      "source": [
        "**Fine tuning (catastrophic forgetting)**<br>\n",
        "In this section of the homework the aim is to demonstrate how, without ad-hoc methodologies, our CNN is unable to learn without dramatically forgetting what it has already been learnt.<br>\n",
        "Operatively, what we do is to perform a training again divided into (ten) steps but without exploiting previous data as before (joint training).\n",
        "What we should observe is a dramatic drop in the perfomances of the network.*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGqCsW-whNpV"
      },
      "source": [
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrA3WhUzuK67"
      },
      "source": [
        "### Fine tuning\n",
        "def sequentialLearning(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    #confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkrMQy2TuUAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7a63d3-d0ef-4cd6-c7de-43a4b545a7e9"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearning(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n",
            "Starting epoch 1/70, LR = [2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.9211840629577637\n",
            "Train step - Step 10, Loss 0.34140273928642273\n",
            "Train step - Step 20, Loss 0.34022167325019836\n",
            "Train step - Step 30, Loss 0.32541051506996155\n",
            "Train epoch - Accuracy: 0.10848484848484849 Loss: 0.40546046273876923 Corrects: 537\n",
            "Starting epoch 2/70, LR = [2]\n",
            "Train step - Step 40, Loss 0.3251975476741791\n",
            "Train step - Step 50, Loss 0.3278141915798187\n",
            "Train step - Step 60, Loss 0.3225761353969574\n",
            "Train step - Step 70, Loss 0.32558757066726685\n",
            "Train epoch - Accuracy: 0.11353535353535353 Loss: 0.3243220260649016 Corrects: 562\n",
            "Starting epoch 3/70, LR = [2]\n",
            "Train step - Step 80, Loss 0.32389697432518005\n",
            "Train step - Step 90, Loss 0.3203265368938446\n",
            "Train step - Step 100, Loss 0.320421427488327\n",
            "Train step - Step 110, Loss 0.3239600360393524\n",
            "Train epoch - Accuracy: 0.13333333333333333 Loss: 0.32235402184303363 Corrects: 660\n",
            "Starting epoch 4/70, LR = [2]\n",
            "Train step - Step 120, Loss 0.32046595215797424\n",
            "Train step - Step 130, Loss 0.3184032142162323\n",
            "Train step - Step 140, Loss 0.3093469440937042\n",
            "Train step - Step 150, Loss 0.3223385214805603\n",
            "Train epoch - Accuracy: 0.15454545454545454 Loss: 0.31788060341218505 Corrects: 765\n",
            "Starting epoch 5/70, LR = [2]\n",
            "Train step - Step 160, Loss 0.3118629455566406\n",
            "Train step - Step 170, Loss 0.3089236915111542\n",
            "Train step - Step 180, Loss 0.30791372060775757\n",
            "Train step - Step 190, Loss 0.30293378233909607\n",
            "Train epoch - Accuracy: 0.1711111111111111 Loss: 0.3097939356408938 Corrects: 847\n",
            "Starting epoch 6/70, LR = [2]\n",
            "Train step - Step 200, Loss 0.3035975992679596\n",
            "Train step - Step 210, Loss 0.31279656291007996\n",
            "Train step - Step 220, Loss 0.3085939586162567\n",
            "Train step - Step 230, Loss 0.3163612484931946\n",
            "Train epoch - Accuracy: 0.18141414141414142 Loss: 0.3036927748208094 Corrects: 898\n",
            "Starting epoch 7/70, LR = [2]\n",
            "Train step - Step 240, Loss 0.29786407947540283\n",
            "Train step - Step 250, Loss 0.30157724022865295\n",
            "Train step - Step 260, Loss 0.30412453413009644\n",
            "Train step - Step 270, Loss 0.2875197231769562\n",
            "Train epoch - Accuracy: 0.2101010101010101 Loss: 0.29844007906287606 Corrects: 1040\n",
            "Starting epoch 8/70, LR = [2]\n",
            "Train step - Step 280, Loss 0.2989465892314911\n",
            "Train step - Step 290, Loss 0.29670432209968567\n",
            "Train step - Step 300, Loss 0.2922362685203552\n",
            "Train step - Step 310, Loss 0.2860437035560608\n",
            "Train epoch - Accuracy: 0.2288888888888889 Loss: 0.2923213817976942 Corrects: 1133\n",
            "Starting epoch 9/70, LR = [2]\n",
            "Train step - Step 320, Loss 0.2928338944911957\n",
            "Train step - Step 330, Loss 0.28102874755859375\n",
            "Train step - Step 340, Loss 0.2876463830471039\n",
            "Train step - Step 350, Loss 0.28951215744018555\n",
            "Train epoch - Accuracy: 0.2474747474747475 Loss: 0.28823454905037926 Corrects: 1225\n",
            "Starting epoch 10/70, LR = [2]\n",
            "Train step - Step 360, Loss 0.2806396782398224\n",
            "Train step - Step 370, Loss 0.2779552638530731\n",
            "Train step - Step 380, Loss 0.2751210629940033\n",
            "Train epoch - Accuracy: 0.28525252525252526 Loss: 0.28011932470581746 Corrects: 1412\n",
            "Starting epoch 11/70, LR = [2]\n",
            "Train step - Step 390, Loss 0.2609812319278717\n",
            "Train step - Step 400, Loss 0.27082571387290955\n",
            "Train step - Step 410, Loss 0.27828365564346313\n",
            "Train step - Step 420, Loss 0.2693363130092621\n",
            "Train epoch - Accuracy: 0.29535353535353537 Loss: 0.27704182930666993 Corrects: 1462\n",
            "Starting epoch 12/70, LR = [2]\n",
            "Train step - Step 430, Loss 0.2722947597503662\n",
            "Train step - Step 440, Loss 0.2698074281215668\n",
            "Train step - Step 450, Loss 0.259848028421402\n",
            "Train step - Step 460, Loss 0.26140961050987244\n",
            "Train epoch - Accuracy: 0.31676767676767675 Loss: 0.27061162152675666 Corrects: 1568\n",
            "Starting epoch 13/70, LR = [2]\n",
            "Train step - Step 470, Loss 0.2708894908428192\n",
            "Train step - Step 480, Loss 0.26382723450660706\n",
            "Train step - Step 490, Loss 0.27001485228538513\n",
            "Train step - Step 500, Loss 0.2540643811225891\n",
            "Train epoch - Accuracy: 0.34525252525252526 Loss: 0.2651673222551442 Corrects: 1709\n",
            "Starting epoch 14/70, LR = [2]\n",
            "Train step - Step 510, Loss 0.25809574127197266\n",
            "Train step - Step 520, Loss 0.25142747163772583\n",
            "Train step - Step 530, Loss 0.27133381366729736\n",
            "Train step - Step 540, Loss 0.2654460370540619\n",
            "Train epoch - Accuracy: 0.3606060606060606 Loss: 0.2596802113152514 Corrects: 1785\n",
            "Starting epoch 15/70, LR = [2]\n",
            "Train step - Step 550, Loss 0.2478029727935791\n",
            "Train step - Step 560, Loss 0.2560276985168457\n",
            "Train step - Step 570, Loss 0.25235483050346375\n",
            "Train step - Step 580, Loss 0.2546052038669586\n",
            "Train epoch - Accuracy: 0.39636363636363636 Loss: 0.25101311873305926 Corrects: 1962\n",
            "Starting epoch 16/70, LR = [2]\n",
            "Train step - Step 590, Loss 0.26660609245300293\n",
            "Train step - Step 600, Loss 0.24717791378498077\n",
            "Train step - Step 610, Loss 0.25263550877571106\n",
            "Train step - Step 620, Loss 0.262549489736557\n",
            "Train epoch - Accuracy: 0.4016161616161616 Loss: 0.24792916194357054 Corrects: 1988\n",
            "Starting epoch 17/70, LR = [2]\n",
            "Train step - Step 630, Loss 0.26598936319351196\n",
            "Train step - Step 640, Loss 0.24016013741493225\n",
            "Train step - Step 650, Loss 0.23943209648132324\n",
            "Train step - Step 660, Loss 0.24597172439098358\n",
            "Train epoch - Accuracy: 0.4218181818181818 Loss: 0.24107893112933998 Corrects: 2088\n",
            "Starting epoch 18/70, LR = [2]\n",
            "Train step - Step 670, Loss 0.25691166520118713\n",
            "Train step - Step 680, Loss 0.237123042345047\n",
            "Train step - Step 690, Loss 0.22199776768684387\n",
            "Train step - Step 700, Loss 0.2367343157529831\n",
            "Train epoch - Accuracy: 0.42686868686868684 Loss: 0.23812653593342714 Corrects: 2113\n",
            "Starting epoch 19/70, LR = [2]\n",
            "Train step - Step 710, Loss 0.24539779126644135\n",
            "Train step - Step 720, Loss 0.23822632431983948\n",
            "Train step - Step 730, Loss 0.23499996960163116\n",
            "Train step - Step 740, Loss 0.2121802717447281\n",
            "Train epoch - Accuracy: 0.4531313131313131 Loss: 0.22918902042538228 Corrects: 2243\n",
            "Starting epoch 20/70, LR = [2]\n",
            "Train step - Step 750, Loss 0.23111192882061005\n",
            "Train step - Step 760, Loss 0.22544649243354797\n",
            "Train step - Step 770, Loss 0.2362002432346344\n",
            "Train epoch - Accuracy: 0.45555555555555555 Loss: 0.22781981507937113 Corrects: 2255\n",
            "Starting epoch 21/70, LR = [2]\n",
            "Train step - Step 780, Loss 0.23984365165233612\n",
            "Train step - Step 790, Loss 0.22111573815345764\n",
            "Train step - Step 800, Loss 0.2315657138824463\n",
            "Train step - Step 810, Loss 0.21379409730434418\n",
            "Train epoch - Accuracy: 0.48545454545454547 Loss: 0.22350297873068337 Corrects: 2403\n",
            "Starting epoch 22/70, LR = [2]\n",
            "Train step - Step 820, Loss 0.21566009521484375\n",
            "Train step - Step 830, Loss 0.22208939492702484\n",
            "Train step - Step 840, Loss 0.20342917740345\n",
            "Train step - Step 850, Loss 0.2398548573255539\n",
            "Train epoch - Accuracy: 0.5012121212121212 Loss: 0.217397460497991 Corrects: 2481\n",
            "Starting epoch 23/70, LR = [2]\n",
            "Train step - Step 860, Loss 0.2232108861207962\n",
            "Train step - Step 870, Loss 0.21444804966449738\n",
            "Train step - Step 880, Loss 0.19692884385585785\n",
            "Train step - Step 890, Loss 0.17631562054157257\n",
            "Train epoch - Accuracy: 0.5183838383838384 Loss: 0.21025170869899518 Corrects: 2566\n",
            "Starting epoch 24/70, LR = [2]\n",
            "Train step - Step 900, Loss 0.192627415060997\n",
            "Train step - Step 910, Loss 0.22372391819953918\n",
            "Train step - Step 920, Loss 0.21769900619983673\n",
            "Train step - Step 930, Loss 0.183990940451622\n",
            "Train epoch - Accuracy: 0.5402020202020202 Loss: 0.2045454834808003 Corrects: 2674\n",
            "Starting epoch 25/70, LR = [2]\n",
            "Train step - Step 940, Loss 0.197311669588089\n",
            "Train step - Step 950, Loss 0.18666747212409973\n",
            "Train step - Step 960, Loss 0.20079366862773895\n",
            "Train step - Step 970, Loss 0.214076429605484\n",
            "Train epoch - Accuracy: 0.5591919191919192 Loss: 0.19913758682482172 Corrects: 2768\n",
            "Starting epoch 26/70, LR = [2]\n",
            "Train step - Step 980, Loss 0.19209514558315277\n",
            "Train step - Step 990, Loss 0.18159881234169006\n",
            "Train step - Step 1000, Loss 0.1814560890197754\n",
            "Train step - Step 1010, Loss 0.1976129561662674\n",
            "Train epoch - Accuracy: 0.573939393939394 Loss: 0.1941386775898211 Corrects: 2841\n",
            "Starting epoch 27/70, LR = [2]\n",
            "Train step - Step 1020, Loss 0.17656096816062927\n",
            "Train step - Step 1030, Loss 0.17534025013446808\n",
            "Train step - Step 1040, Loss 0.17545466125011444\n",
            "Train step - Step 1050, Loss 0.23665539920330048\n",
            "Train epoch - Accuracy: 0.5874747474747475 Loss: 0.18939453993782854 Corrects: 2908\n",
            "Starting epoch 28/70, LR = [2]\n",
            "Train step - Step 1060, Loss 0.17773354053497314\n",
            "Train step - Step 1070, Loss 0.17403924465179443\n",
            "Train step - Step 1080, Loss 0.21619093418121338\n",
            "Train step - Step 1090, Loss 0.18047931790351868\n",
            "Train epoch - Accuracy: 0.6056565656565657 Loss: 0.18205074245279484 Corrects: 2998\n",
            "Starting epoch 29/70, LR = [2]\n",
            "Train step - Step 1100, Loss 0.21092239022254944\n",
            "Train step - Step 1110, Loss 0.16916458308696747\n",
            "Train step - Step 1120, Loss 0.17385850846767426\n",
            "Train step - Step 1130, Loss 0.19755226373672485\n",
            "Train epoch - Accuracy: 0.6113131313131314 Loss: 0.18216054095162285 Corrects: 3026\n",
            "Starting epoch 30/70, LR = [2]\n",
            "Train step - Step 1140, Loss 0.17935490608215332\n",
            "Train step - Step 1150, Loss 0.1687288135290146\n",
            "Train step - Step 1160, Loss 0.17799246311187744\n",
            "Train epoch - Accuracy: 0.6426262626262627 Loss: 0.16875124374423364 Corrects: 3181\n",
            "Starting epoch 31/70, LR = [2]\n",
            "Train step - Step 1170, Loss 0.1521521359682083\n",
            "Train step - Step 1180, Loss 0.1736326664686203\n",
            "Train step - Step 1190, Loss 0.16831989586353302\n",
            "Train step - Step 1200, Loss 0.18084006011486053\n",
            "Train epoch - Accuracy: 0.6339393939393939 Loss: 0.17049233719556017 Corrects: 3138\n",
            "Starting epoch 32/70, LR = [2]\n",
            "Train step - Step 1210, Loss 0.17353007197380066\n",
            "Train step - Step 1220, Loss 0.18193235993385315\n",
            "Train step - Step 1230, Loss 0.17781397700309753\n",
            "Train step - Step 1240, Loss 0.15604524314403534\n",
            "Train epoch - Accuracy: 0.6486868686868686 Loss: 0.16654477250696434 Corrects: 3211\n",
            "Starting epoch 33/70, LR = [2]\n",
            "Train step - Step 1250, Loss 0.1828964203596115\n",
            "Train step - Step 1260, Loss 0.1820468157529831\n",
            "Train step - Step 1270, Loss 0.20315678417682648\n",
            "Train step - Step 1280, Loss 0.14744552969932556\n",
            "Train epoch - Accuracy: 0.6327272727272727 Loss: 0.16973948715311107 Corrects: 3132\n",
            "Starting epoch 34/70, LR = [2]\n",
            "Train step - Step 1290, Loss 0.15109370648860931\n",
            "Train step - Step 1300, Loss 0.1576143354177475\n",
            "Train step - Step 1310, Loss 0.1519923061132431\n",
            "Train step - Step 1320, Loss 0.1595785915851593\n",
            "Train epoch - Accuracy: 0.6539393939393939 Loss: 0.16271672035708573 Corrects: 3237\n",
            "Starting epoch 35/70, LR = [2]\n",
            "Train step - Step 1330, Loss 0.1494823396205902\n",
            "Train step - Step 1340, Loss 0.17393873631954193\n",
            "Train step - Step 1350, Loss 0.1620786041021347\n",
            "Train step - Step 1360, Loss 0.1750020533800125\n",
            "Train epoch - Accuracy: 0.6703030303030303 Loss: 0.1565762852478509 Corrects: 3318\n",
            "Starting epoch 36/70, LR = [2]\n",
            "Train step - Step 1370, Loss 0.14456680417060852\n",
            "Train step - Step 1380, Loss 0.14493322372436523\n",
            "Train step - Step 1390, Loss 0.19187872111797333\n",
            "Train step - Step 1400, Loss 0.14658381044864655\n",
            "Train epoch - Accuracy: 0.6783838383838384 Loss: 0.15324477959160854 Corrects: 3358\n",
            "Starting epoch 37/70, LR = [2]\n",
            "Train step - Step 1410, Loss 0.1696283221244812\n",
            "Train step - Step 1420, Loss 0.1318415403366089\n",
            "Train step - Step 1430, Loss 0.13416974246501923\n",
            "Train step - Step 1440, Loss 0.14998626708984375\n",
            "Train epoch - Accuracy: 0.694949494949495 Loss: 0.14604556291994422 Corrects: 3440\n",
            "Starting epoch 38/70, LR = [2]\n",
            "Train step - Step 1450, Loss 0.17098991572856903\n",
            "Train step - Step 1460, Loss 0.16216576099395752\n",
            "Train step - Step 1470, Loss 0.1596590131521225\n",
            "Train step - Step 1480, Loss 0.16245955228805542\n",
            "Train epoch - Accuracy: 0.6995959595959595 Loss: 0.1465634564558665 Corrects: 3463\n",
            "Starting epoch 39/70, LR = [2]\n",
            "Train step - Step 1490, Loss 0.13130365312099457\n",
            "Train step - Step 1500, Loss 0.14412157237529755\n",
            "Train step - Step 1510, Loss 0.1177118644118309\n",
            "Train step - Step 1520, Loss 0.13980858027935028\n",
            "Train epoch - Accuracy: 0.7185858585858586 Loss: 0.13443769595237692 Corrects: 3557\n",
            "Starting epoch 40/70, LR = [2]\n",
            "Train step - Step 1530, Loss 0.151846781373024\n",
            "Train step - Step 1540, Loss 0.15226711332798004\n",
            "Train step - Step 1550, Loss 0.13552144169807434\n",
            "Train epoch - Accuracy: 0.7248484848484849 Loss: 0.13413021577127052 Corrects: 3588\n",
            "Starting epoch 41/70, LR = [2]\n",
            "Train step - Step 1560, Loss 0.16038839519023895\n",
            "Train step - Step 1570, Loss 0.12939448654651642\n",
            "Train step - Step 1580, Loss 0.1341617852449417\n",
            "Train step - Step 1590, Loss 0.13197647035121918\n",
            "Train epoch - Accuracy: 0.7179797979797979 Loss: 0.13366501911722048 Corrects: 3554\n",
            "Starting epoch 42/70, LR = [2]\n",
            "Train step - Step 1600, Loss 0.13529649376869202\n",
            "Train step - Step 1610, Loss 0.1188676580786705\n",
            "Train step - Step 1620, Loss 0.10934042185544968\n",
            "Train step - Step 1630, Loss 0.12094900757074356\n",
            "Train epoch - Accuracy: 0.7418181818181818 Loss: 0.12774312485950162 Corrects: 3672\n",
            "Starting epoch 43/70, LR = [2]\n",
            "Train step - Step 1640, Loss 0.11827576160430908\n",
            "Train step - Step 1650, Loss 0.13239721953868866\n",
            "Train step - Step 1660, Loss 0.12316296249628067\n",
            "Train step - Step 1670, Loss 0.13833890855312347\n",
            "Train epoch - Accuracy: 0.7357575757575757 Loss: 0.13071673421245633 Corrects: 3642\n",
            "Starting epoch 44/70, LR = [2]\n",
            "Train step - Step 1680, Loss 0.1242581233382225\n",
            "Train step - Step 1690, Loss 0.11186843365430832\n",
            "Train step - Step 1700, Loss 0.1161305233836174\n",
            "Train step - Step 1710, Loss 0.13154743611812592\n",
            "Train epoch - Accuracy: 0.7555555555555555 Loss: 0.12068857941362593 Corrects: 3740\n",
            "Starting epoch 45/70, LR = [2]\n",
            "Train step - Step 1720, Loss 0.12689942121505737\n",
            "Train step - Step 1730, Loss 0.11538457870483398\n",
            "Train step - Step 1740, Loss 0.12507402896881104\n",
            "Train step - Step 1750, Loss 0.0942222997546196\n",
            "Train epoch - Accuracy: 0.7642424242424243 Loss: 0.11970963375435935 Corrects: 3783\n",
            "Starting epoch 46/70, LR = [2]\n",
            "Train step - Step 1760, Loss 0.12364373356103897\n",
            "Train step - Step 1770, Loss 0.12092568725347519\n",
            "Train step - Step 1780, Loss 0.12207692116498947\n",
            "Train step - Step 1790, Loss 0.10089128464460373\n",
            "Train epoch - Accuracy: 0.7553535353535353 Loss: 0.11991526229212983 Corrects: 3739\n",
            "Starting epoch 47/70, LR = [2]\n",
            "Train step - Step 1800, Loss 0.11019191890954971\n",
            "Train step - Step 1810, Loss 0.08919762820005417\n",
            "Train step - Step 1820, Loss 0.10839565098285675\n",
            "Train step - Step 1830, Loss 0.1184498593211174\n",
            "Train epoch - Accuracy: 0.7808080808080808 Loss: 0.11312339099669698 Corrects: 3865\n",
            "Starting epoch 48/70, LR = [2]\n",
            "Train step - Step 1840, Loss 0.11689907312393188\n",
            "Train step - Step 1850, Loss 0.09606245905160904\n",
            "Train step - Step 1860, Loss 0.1357046663761139\n",
            "Train step - Step 1870, Loss 0.13148656487464905\n",
            "Train epoch - Accuracy: 0.7656565656565657 Loss: 0.11773318169393925 Corrects: 3790\n",
            "Starting epoch 49/70, LR = [2]\n",
            "Train step - Step 1880, Loss 0.10904581844806671\n",
            "Train step - Step 1890, Loss 0.10921464115381241\n",
            "Train step - Step 1900, Loss 0.0934833213686943\n",
            "Train step - Step 1910, Loss 0.14015504717826843\n",
            "Train epoch - Accuracy: 0.7846464646464646 Loss: 0.10799128846688703 Corrects: 3884\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "Train step - Step 1920, Loss 0.08136671781539917\n",
            "Train step - Step 1930, Loss 0.10230765491724014\n",
            "Train step - Step 1940, Loss 0.09946916252374649\n",
            "Train epoch - Accuracy: 0.8264646464646465 Loss: 0.0904282508323891 Corrects: 4091\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "Train step - Step 1950, Loss 0.06063978001475334\n",
            "Train step - Step 1960, Loss 0.06979892402887344\n",
            "Train step - Step 1970, Loss 0.07197251915931702\n",
            "Train step - Step 1980, Loss 0.07393345981836319\n",
            "Train epoch - Accuracy: 0.8466666666666667 Loss: 0.07871465045695353 Corrects: 4191\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "Train step - Step 1990, Loss 0.0736340880393982\n",
            "Train step - Step 2000, Loss 0.0805356353521347\n",
            "Train step - Step 2010, Loss 0.08990287780761719\n",
            "Train step - Step 2020, Loss 0.05568398907780647\n",
            "Train epoch - Accuracy: 0.8533333333333334 Loss: 0.07693365559433446 Corrects: 4224\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "Train step - Step 2030, Loss 0.08451642096042633\n",
            "Train step - Step 2040, Loss 0.07027894258499146\n",
            "Train step - Step 2050, Loss 0.06242473050951958\n",
            "Train step - Step 2060, Loss 0.0639040470123291\n",
            "Train epoch - Accuracy: 0.8549494949494949 Loss: 0.07530596115673431 Corrects: 4232\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "Train step - Step 2070, Loss 0.06889691203832626\n",
            "Train step - Step 2080, Loss 0.08650829643011093\n",
            "Train step - Step 2090, Loss 0.07814811915159225\n",
            "Train step - Step 2100, Loss 0.06135235354304314\n",
            "Train epoch - Accuracy: 0.861010101010101 Loss: 0.07222034187328936 Corrects: 4262\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "Train step - Step 2110, Loss 0.08792568743228912\n",
            "Train step - Step 2120, Loss 0.07306166738271713\n",
            "Train step - Step 2130, Loss 0.08159246295690536\n",
            "Train step - Step 2140, Loss 0.07074663788080215\n",
            "Train epoch - Accuracy: 0.8577777777777778 Loss: 0.07274439417954646 Corrects: 4246\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "Train step - Step 2150, Loss 0.05363577604293823\n",
            "Train step - Step 2160, Loss 0.06026965379714966\n",
            "Train step - Step 2170, Loss 0.08138696104288101\n",
            "Train step - Step 2180, Loss 0.06914974749088287\n",
            "Train epoch - Accuracy: 0.8715151515151515 Loss: 0.06993251350491939 Corrects: 4314\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "Train step - Step 2190, Loss 0.08972673118114471\n",
            "Train step - Step 2200, Loss 0.07111813873052597\n",
            "Train step - Step 2210, Loss 0.06851660460233688\n",
            "Train step - Step 2220, Loss 0.08266631513834\n",
            "Train epoch - Accuracy: 0.8741414141414141 Loss: 0.06783828660093173 Corrects: 4327\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "Train step - Step 2230, Loss 0.0799349769949913\n",
            "Train step - Step 2240, Loss 0.0874258428812027\n",
            "Train step - Step 2250, Loss 0.0822223499417305\n",
            "Train step - Step 2260, Loss 0.06154756620526314\n",
            "Train epoch - Accuracy: 0.8751515151515151 Loss: 0.06617599480982983 Corrects: 4332\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "Train step - Step 2270, Loss 0.058548856526613235\n",
            "Train step - Step 2280, Loss 0.06295845657587051\n",
            "Train step - Step 2290, Loss 0.08616917580366135\n",
            "Train step - Step 2300, Loss 0.06542102992534637\n",
            "Train epoch - Accuracy: 0.8696969696969697 Loss: 0.06627010822898209 Corrects: 4305\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "Train step - Step 2310, Loss 0.057194363325834274\n",
            "Train step - Step 2320, Loss 0.05675153806805611\n",
            "Train step - Step 2330, Loss 0.04538669064640999\n",
            "Train epoch - Accuracy: 0.8858585858585859 Loss: 0.06180591569103376 Corrects: 4385\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "Train step - Step 2340, Loss 0.04903285577893257\n",
            "Train step - Step 2350, Loss 0.04044211655855179\n",
            "Train step - Step 2360, Loss 0.053505729883909225\n",
            "Train step - Step 2370, Loss 0.0533476360142231\n",
            "Train epoch - Accuracy: 0.8783838383838384 Loss: 0.06358992568772248 Corrects: 4348\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "Train step - Step 2380, Loss 0.05685610696673393\n",
            "Train step - Step 2390, Loss 0.06673236191272736\n",
            "Train step - Step 2400, Loss 0.06992670148611069\n",
            "Train step - Step 2410, Loss 0.0738740786910057\n",
            "Train epoch - Accuracy: 0.8919191919191919 Loss: 0.05740956557218475 Corrects: 4415\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "Train step - Step 2420, Loss 0.0641394630074501\n",
            "Train step - Step 2430, Loss 0.0516042485833168\n",
            "Train step - Step 2440, Loss 0.0572042241692543\n",
            "Train step - Step 2450, Loss 0.06299494951963425\n",
            "Train epoch - Accuracy: 0.8860606060606061 Loss: 0.06120107495423519 Corrects: 4386\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "Train step - Step 2460, Loss 0.03671155497431755\n",
            "Train step - Step 2470, Loss 0.04988250881433487\n",
            "Train step - Step 2480, Loss 0.045535050332546234\n",
            "Train step - Step 2490, Loss 0.04312460497021675\n",
            "Train epoch - Accuracy: 0.9038383838383839 Loss: 0.05366548012001346 Corrects: 4474\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2500, Loss 0.05510486289858818\n",
            "Train step - Step 2510, Loss 0.03440647944808006\n",
            "Train step - Step 2520, Loss 0.055795300751924515\n",
            "Train step - Step 2530, Loss 0.029772544279694557\n",
            "Train epoch - Accuracy: 0.9084848484848485 Loss: 0.05110390697013248 Corrects: 4497\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2540, Loss 0.05119818076491356\n",
            "Train step - Step 2550, Loss 0.06064973026514053\n",
            "Train step - Step 2560, Loss 0.053173650056123734\n",
            "Train step - Step 2570, Loss 0.03253941610455513\n",
            "Train epoch - Accuracy: 0.9113131313131313 Loss: 0.04982439072520444 Corrects: 4511\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2580, Loss 0.03895008563995361\n",
            "Train step - Step 2590, Loss 0.06879228353500366\n",
            "Train step - Step 2600, Loss 0.0440681129693985\n",
            "Train step - Step 2610, Loss 0.05733500048518181\n",
            "Train epoch - Accuracy: 0.9086868686868687 Loss: 0.04962053030127227 Corrects: 4498\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2620, Loss 0.047916825860738754\n",
            "Train step - Step 2630, Loss 0.05302795395255089\n",
            "Train step - Step 2640, Loss 0.05506391450762749\n",
            "Train step - Step 2650, Loss 0.04381290823221207\n",
            "Train epoch - Accuracy: 0.9133333333333333 Loss: 0.04866633512305491 Corrects: 4521\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2660, Loss 0.05546491965651512\n",
            "Train step - Step 2670, Loss 0.042862217873334885\n",
            "Train step - Step 2680, Loss 0.06170407682657242\n",
            "Train step - Step 2690, Loss 0.03525175154209137\n",
            "Train epoch - Accuracy: 0.9111111111111111 Loss: 0.048211354456766685 Corrects: 4510\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2700, Loss 0.0459555946290493\n",
            "Train step - Step 2710, Loss 0.0559561513364315\n",
            "Train step - Step 2720, Loss 0.04578403756022453\n",
            "Train epoch - Accuracy: 0.9151515151515152 Loss: 0.04733476932151149 Corrects: 4530\n",
            "Training finished in 405.10724091529846 seconds\n",
            "EVALUATION:  0.84 0.11075635254383087\n",
            "TEST GROUP:  0.796\n",
            "TEST ALL:  0.796\n",
            "GROUP:  2\n",
            "Starting epoch 1/70, LR = [2]\n",
            "Train step - Step 0, Loss 0.4769144058227539\n",
            "Train step - Step 10, Loss 0.19764645397663116\n",
            "Train step - Step 20, Loss 0.14605854451656342\n",
            "Train step - Step 30, Loss 0.11248507350683212\n",
            "Train epoch - Accuracy: 0.32242424242424245 Loss: 0.16461746936795688 Corrects: 1596\n",
            "Starting epoch 2/70, LR = [2]\n",
            "Train step - Step 40, Loss 0.10846634209156036\n",
            "Train step - Step 50, Loss 0.09180768579244614\n",
            "Train step - Step 60, Loss 0.08857979625463486\n",
            "Train step - Step 70, Loss 0.07825576514005661\n",
            "Train epoch - Accuracy: 0.5842424242424242 Loss: 0.09358230752475334 Corrects: 2892\n",
            "Starting epoch 3/70, LR = [2]\n",
            "Train step - Step 80, Loss 0.08682646602392197\n",
            "Train step - Step 90, Loss 0.07308267802000046\n",
            "Train step - Step 100, Loss 0.07375245541334152\n",
            "Train step - Step 110, Loss 0.08429483324289322\n",
            "Train epoch - Accuracy: 0.6688888888888889 Loss: 0.07771671366209935 Corrects: 3311\n",
            "Starting epoch 4/70, LR = [2]\n",
            "Train step - Step 120, Loss 0.08156634867191315\n",
            "Train step - Step 130, Loss 0.06025397405028343\n",
            "Train step - Step 140, Loss 0.08101517707109451\n",
            "Train step - Step 150, Loss 0.055223047733306885\n",
            "Train epoch - Accuracy: 0.7125252525252526 Loss: 0.06908257231266812 Corrects: 3527\n",
            "Starting epoch 5/70, LR = [2]\n",
            "Train step - Step 160, Loss 0.06501972675323486\n",
            "Train step - Step 170, Loss 0.06341331452131271\n",
            "Train step - Step 180, Loss 0.07932821661233902\n",
            "Train step - Step 190, Loss 0.06017179414629936\n",
            "Train epoch - Accuracy: 0.7355555555555555 Loss: 0.06491885566350185 Corrects: 3641\n",
            "Starting epoch 6/70, LR = [2]\n",
            "Train step - Step 200, Loss 0.05718206241726875\n",
            "Train step - Step 210, Loss 0.0680788904428482\n",
            "Train step - Step 220, Loss 0.0536324568092823\n",
            "Train step - Step 230, Loss 0.0685821995139122\n",
            "Train epoch - Accuracy: 0.7571717171717172 Loss: 0.060138380985067345 Corrects: 3748\n",
            "Starting epoch 7/70, LR = [2]\n",
            "Train step - Step 240, Loss 0.05626728758215904\n",
            "Train step - Step 250, Loss 0.062252551317214966\n",
            "Train step - Step 260, Loss 0.05267767235636711\n",
            "Train step - Step 270, Loss 0.05900236591696739\n",
            "Train epoch - Accuracy: 0.7707070707070707 Loss: 0.05592268928132876 Corrects: 3815\n",
            "Starting epoch 8/70, LR = [2]\n",
            "Train step - Step 280, Loss 0.04745390638709068\n",
            "Train step - Step 290, Loss 0.049502771347761154\n",
            "Train step - Step 300, Loss 0.05265326425433159\n",
            "Train step - Step 310, Loss 0.05314977839589119\n",
            "Train epoch - Accuracy: 0.7953535353535354 Loss: 0.05118447907645293 Corrects: 3937\n",
            "Starting epoch 9/70, LR = [2]\n",
            "Train step - Step 320, Loss 0.04360775277018547\n",
            "Train step - Step 330, Loss 0.057764191180467606\n",
            "Train step - Step 340, Loss 0.055010464042425156\n",
            "Train step - Step 350, Loss 0.0335821695625782\n",
            "Train epoch - Accuracy: 0.7951515151515152 Loss: 0.05116476055197042 Corrects: 3936\n",
            "Starting epoch 10/70, LR = [2]\n",
            "Train step - Step 360, Loss 0.04438241198658943\n",
            "Train step - Step 370, Loss 0.04527771472930908\n",
            "Train step - Step 380, Loss 0.04218460991978645\n",
            "Train epoch - Accuracy: 0.8084848484848485 Loss: 0.048494403455594574 Corrects: 4002\n",
            "Starting epoch 11/70, LR = [2]\n",
            "Train step - Step 390, Loss 0.04532550647854805\n",
            "Train step - Step 400, Loss 0.0434303879737854\n",
            "Train step - Step 410, Loss 0.03663644194602966\n",
            "Train step - Step 420, Loss 0.04979988932609558\n",
            "Train epoch - Accuracy: 0.8252525252525252 Loss: 0.04484276877810257 Corrects: 4085\n",
            "Starting epoch 12/70, LR = [2]\n",
            "Train step - Step 430, Loss 0.05436133220791817\n",
            "Train step - Step 440, Loss 0.050081826746463776\n",
            "Train step - Step 450, Loss 0.04486478865146637\n",
            "Train step - Step 460, Loss 0.054173197597265244\n",
            "Train epoch - Accuracy: 0.8222222222222222 Loss: 0.044893093458329786 Corrects: 4070\n",
            "Starting epoch 13/70, LR = [2]\n",
            "Train step - Step 470, Loss 0.050079233944416046\n",
            "Train step - Step 480, Loss 0.03727159649133682\n",
            "Train step - Step 490, Loss 0.051517095416784286\n",
            "Train step - Step 500, Loss 0.04767128452658653\n",
            "Train epoch - Accuracy: 0.8333333333333334 Loss: 0.042852554647910476 Corrects: 4125\n",
            "Starting epoch 14/70, LR = [2]\n",
            "Train step - Step 510, Loss 0.03415818139910698\n",
            "Train step - Step 520, Loss 0.03025255911052227\n",
            "Train step - Step 530, Loss 0.036486439406871796\n",
            "Train step - Step 540, Loss 0.05518551543354988\n",
            "Train epoch - Accuracy: 0.8404040404040404 Loss: 0.04063014476437761 Corrects: 4160\n",
            "Starting epoch 15/70, LR = [2]\n",
            "Train step - Step 550, Loss 0.047788675874471664\n",
            "Train step - Step 560, Loss 0.03882836923003197\n",
            "Train step - Step 570, Loss 0.033943209797143936\n",
            "Train step - Step 580, Loss 0.04741695523262024\n",
            "Train epoch - Accuracy: 0.8466666666666667 Loss: 0.03980472974253423 Corrects: 4191\n",
            "Starting epoch 16/70, LR = [2]\n",
            "Train step - Step 590, Loss 0.042351290583610535\n",
            "Train step - Step 600, Loss 0.028192145749926567\n",
            "Train step - Step 610, Loss 0.03368908539414406\n",
            "Train step - Step 620, Loss 0.03923437371850014\n",
            "Train epoch - Accuracy: 0.8501010101010101 Loss: 0.03912822602373181 Corrects: 4208\n",
            "Starting epoch 17/70, LR = [2]\n",
            "Train step - Step 630, Loss 0.03418324515223503\n",
            "Train step - Step 640, Loss 0.02861659601330757\n",
            "Train step - Step 650, Loss 0.030611654743552208\n",
            "Train step - Step 660, Loss 0.04059217870235443\n",
            "Train epoch - Accuracy: 0.8644444444444445 Loss: 0.036099344540123986 Corrects: 4279\n",
            "Starting epoch 18/70, LR = [2]\n",
            "Train step - Step 670, Loss 0.0317201241850853\n",
            "Train step - Step 680, Loss 0.034857071936130524\n",
            "Train step - Step 690, Loss 0.032188087701797485\n",
            "Train step - Step 700, Loss 0.027810392901301384\n",
            "Train epoch - Accuracy: 0.8654545454545455 Loss: 0.03427585282410034 Corrects: 4284\n",
            "Starting epoch 19/70, LR = [2]\n",
            "Train step - Step 710, Loss 0.032222069799900055\n",
            "Train step - Step 720, Loss 0.03239568695425987\n",
            "Train step - Step 730, Loss 0.04506251588463783\n",
            "Train step - Step 740, Loss 0.051799945533275604\n",
            "Train epoch - Accuracy: 0.8674747474747475 Loss: 0.03437825067175759 Corrects: 4294\n",
            "Starting epoch 20/70, LR = [2]\n",
            "Train step - Step 750, Loss 0.03373996540904045\n",
            "Train step - Step 760, Loss 0.036413438618183136\n",
            "Train step - Step 770, Loss 0.03542157635092735\n",
            "Train epoch - Accuracy: 0.8642424242424243 Loss: 0.03381920652708622 Corrects: 4278\n",
            "Starting epoch 21/70, LR = [2]\n",
            "Train step - Step 780, Loss 0.025339728221297264\n",
            "Train step - Step 790, Loss 0.04083995893597603\n",
            "Train step - Step 800, Loss 0.03455087170004845\n",
            "Train step - Step 810, Loss 0.027498586103320122\n",
            "Train epoch - Accuracy: 0.8743434343434343 Loss: 0.032195724914170276 Corrects: 4328\n",
            "Starting epoch 22/70, LR = [2]\n",
            "Train step - Step 820, Loss 0.02824343554675579\n",
            "Train step - Step 830, Loss 0.027649808675050735\n",
            "Train step - Step 840, Loss 0.03812391683459282\n",
            "Train step - Step 850, Loss 0.025193853303790092\n",
            "Train epoch - Accuracy: 0.8878787878787879 Loss: 0.02960661795931031 Corrects: 4395\n",
            "Starting epoch 23/70, LR = [2]\n",
            "Train step - Step 860, Loss 0.03202139586210251\n",
            "Train step - Step 870, Loss 0.026698220521211624\n",
            "Train step - Step 880, Loss 0.026326024904847145\n",
            "Train step - Step 890, Loss 0.026059795171022415\n",
            "Train epoch - Accuracy: 0.8757575757575757 Loss: 0.03153236080826533 Corrects: 4335\n",
            "Starting epoch 24/70, LR = [2]\n",
            "Train step - Step 900, Loss 0.02572263590991497\n",
            "Train step - Step 910, Loss 0.04826858267188072\n",
            "Train step - Step 920, Loss 0.028665995225310326\n",
            "Train step - Step 930, Loss 0.027406318113207817\n",
            "Train epoch - Accuracy: 0.8876767676767677 Loss: 0.029529889067164576 Corrects: 4394\n",
            "Starting epoch 25/70, LR = [2]\n",
            "Train step - Step 940, Loss 0.03496396169066429\n",
            "Train step - Step 950, Loss 0.020846372470259666\n",
            "Train step - Step 960, Loss 0.02984238974750042\n",
            "Train step - Step 970, Loss 0.04724917933344841\n",
            "Train epoch - Accuracy: 0.888080808080808 Loss: 0.029254832382153984 Corrects: 4396\n",
            "Starting epoch 26/70, LR = [2]\n",
            "Train step - Step 980, Loss 0.029129033908247948\n",
            "Train step - Step 990, Loss 0.020738476887345314\n",
            "Train step - Step 1000, Loss 0.02475397102534771\n",
            "Train step - Step 1010, Loss 0.021068567410111427\n",
            "Train epoch - Accuracy: 0.8931313131313131 Loss: 0.027895493272579078 Corrects: 4421\n",
            "Starting epoch 27/70, LR = [2]\n",
            "Train step - Step 1020, Loss 0.025379640981554985\n",
            "Train step - Step 1030, Loss 0.017822222784161568\n",
            "Train step - Step 1040, Loss 0.027346277609467506\n",
            "Train step - Step 1050, Loss 0.028627783060073853\n",
            "Train epoch - Accuracy: 0.9004040404040404 Loss: 0.0268053469272575 Corrects: 4457\n",
            "Starting epoch 28/70, LR = [2]\n",
            "Train step - Step 1060, Loss 0.03493427485227585\n",
            "Train step - Step 1070, Loss 0.04028122499585152\n",
            "Train step - Step 1080, Loss 0.026685113087296486\n",
            "Train step - Step 1090, Loss 0.020025057718157768\n",
            "Train epoch - Accuracy: 0.9036363636363637 Loss: 0.025518704028894203 Corrects: 4473\n",
            "Starting epoch 29/70, LR = [2]\n",
            "Train step - Step 1100, Loss 0.021280739456415176\n",
            "Train step - Step 1110, Loss 0.033921390771865845\n",
            "Train step - Step 1120, Loss 0.018766338005661964\n",
            "Train step - Step 1130, Loss 0.022698411718010902\n",
            "Train epoch - Accuracy: 0.908080808080808 Loss: 0.024714573024950847 Corrects: 4495\n",
            "Starting epoch 30/70, LR = [2]\n",
            "Train step - Step 1140, Loss 0.026639515534043312\n",
            "Train step - Step 1150, Loss 0.026185885071754456\n",
            "Train step - Step 1160, Loss 0.02083253674209118\n",
            "Train epoch - Accuracy: 0.9006060606060606 Loss: 0.026894048227354733 Corrects: 4458\n",
            "Starting epoch 31/70, LR = [2]\n",
            "Train step - Step 1170, Loss 0.016777968034148216\n",
            "Train step - Step 1180, Loss 0.026019645854830742\n",
            "Train step - Step 1190, Loss 0.022573942318558693\n",
            "Train step - Step 1200, Loss 0.02383091300725937\n",
            "Train epoch - Accuracy: 0.908080808080808 Loss: 0.02428220019798086 Corrects: 4495\n",
            "Starting epoch 32/70, LR = [2]\n",
            "Train step - Step 1210, Loss 0.021394914016127586\n",
            "Train step - Step 1220, Loss 0.015532893128693104\n",
            "Train step - Step 1230, Loss 0.02217091992497444\n",
            "Train step - Step 1240, Loss 0.027927089482545853\n",
            "Train epoch - Accuracy: 0.9105050505050505 Loss: 0.022648761413916193 Corrects: 4507\n",
            "Starting epoch 33/70, LR = [2]\n",
            "Train step - Step 1250, Loss 0.022227447479963303\n",
            "Train step - Step 1260, Loss 0.029164521023631096\n",
            "Train step - Step 1270, Loss 0.026369420811533928\n",
            "Train step - Step 1280, Loss 0.026390278711915016\n",
            "Train epoch - Accuracy: 0.9133333333333333 Loss: 0.023201623120994278 Corrects: 4521\n",
            "Starting epoch 34/70, LR = [2]\n",
            "Train step - Step 1290, Loss 0.017880449071526527\n",
            "Train step - Step 1300, Loss 0.02266901545226574\n",
            "Train step - Step 1310, Loss 0.021649712696671486\n",
            "Train step - Step 1320, Loss 0.031202265992760658\n",
            "Train epoch - Accuracy: 0.9153535353535354 Loss: 0.023139920047286783 Corrects: 4531\n",
            "Starting epoch 35/70, LR = [2]\n",
            "Train step - Step 1330, Loss 0.021069033071398735\n",
            "Train step - Step 1340, Loss 0.02282351814210415\n",
            "Train step - Step 1350, Loss 0.017381008714437485\n",
            "Train step - Step 1360, Loss 0.031579457223415375\n",
            "Train epoch - Accuracy: 0.9206060606060606 Loss: 0.021425291832649348 Corrects: 4557\n",
            "Starting epoch 36/70, LR = [2]\n",
            "Train step - Step 1370, Loss 0.029062896966934204\n",
            "Train step - Step 1380, Loss 0.0310619305819273\n",
            "Train step - Step 1390, Loss 0.01705905608832836\n",
            "Train step - Step 1400, Loss 0.017788639292120934\n",
            "Train epoch - Accuracy: 0.9153535353535354 Loss: 0.022485374350740453 Corrects: 4531\n",
            "Starting epoch 37/70, LR = [2]\n",
            "Train step - Step 1410, Loss 0.02597673051059246\n",
            "Train step - Step 1420, Loss 0.026760926470160484\n",
            "Train step - Step 1430, Loss 0.023104539141058922\n",
            "Train step - Step 1440, Loss 0.018175393342971802\n",
            "Train epoch - Accuracy: 0.9139393939393939 Loss: 0.02341030614619905 Corrects: 4524\n",
            "Starting epoch 38/70, LR = [2]\n",
            "Train step - Step 1450, Loss 0.021711504086852074\n",
            "Train step - Step 1460, Loss 0.03113389015197754\n",
            "Train step - Step 1470, Loss 0.020343439653515816\n",
            "Train step - Step 1480, Loss 0.014786160551011562\n",
            "Train epoch - Accuracy: 0.926060606060606 Loss: 0.02042124734457695 Corrects: 4584\n",
            "Starting epoch 39/70, LR = [2]\n",
            "Train step - Step 1490, Loss 0.01785793900489807\n",
            "Train step - Step 1500, Loss 0.01779935322701931\n",
            "Train step - Step 1510, Loss 0.018951328471302986\n",
            "Train step - Step 1520, Loss 0.013942232355475426\n",
            "Train epoch - Accuracy: 0.9276767676767677 Loss: 0.02031679606874182 Corrects: 4592\n",
            "Starting epoch 40/70, LR = [2]\n",
            "Train step - Step 1530, Loss 0.01750822179019451\n",
            "Train step - Step 1540, Loss 0.021838683634996414\n",
            "Train step - Step 1550, Loss 0.021723994985222816\n",
            "Train epoch - Accuracy: 0.9294949494949495 Loss: 0.019313639835877854 Corrects: 4601\n",
            "Starting epoch 41/70, LR = [2]\n",
            "Train step - Step 1560, Loss 0.016235506162047386\n",
            "Train step - Step 1570, Loss 0.01789160631597042\n",
            "Train step - Step 1580, Loss 0.0264427550137043\n",
            "Train step - Step 1590, Loss 0.019815001636743546\n",
            "Train epoch - Accuracy: 0.9319191919191919 Loss: 0.018605849515609067 Corrects: 4613\n",
            "Starting epoch 42/70, LR = [2]\n",
            "Train step - Step 1600, Loss 0.015741759911179543\n",
            "Train step - Step 1610, Loss 0.021539339795708656\n",
            "Train step - Step 1620, Loss 0.024371659383177757\n",
            "Train step - Step 1630, Loss 0.013642990961670876\n",
            "Train epoch - Accuracy: 0.9343434343434344 Loss: 0.017836559937533105 Corrects: 4625\n",
            "Starting epoch 43/70, LR = [2]\n",
            "Train step - Step 1640, Loss 0.01285538263618946\n",
            "Train step - Step 1650, Loss 0.013840354047715664\n",
            "Train step - Step 1660, Loss 0.01272102352231741\n",
            "Train step - Step 1670, Loss 0.015863439068198204\n",
            "Train epoch - Accuracy: 0.9454545454545454 Loss: 0.015625851636733672 Corrects: 4680\n",
            "Starting epoch 44/70, LR = [2]\n",
            "Train step - Step 1680, Loss 0.012842664495110512\n",
            "Train step - Step 1690, Loss 0.022635115310549736\n",
            "Train step - Step 1700, Loss 0.011952737346291542\n",
            "Train step - Step 1710, Loss 0.013872307725250721\n",
            "Train epoch - Accuracy: 0.94 Loss: 0.017803750992423355 Corrects: 4653\n",
            "Starting epoch 45/70, LR = [2]\n",
            "Train step - Step 1720, Loss 0.024845533072948456\n",
            "Train step - Step 1730, Loss 0.01631004922091961\n",
            "Train step - Step 1740, Loss 0.018168561160564423\n",
            "Train step - Step 1750, Loss 0.012788079679012299\n",
            "Train epoch - Accuracy: 0.9468686868686869 Loss: 0.01558002495855996 Corrects: 4687\n",
            "Starting epoch 46/70, LR = [2]\n",
            "Train step - Step 1760, Loss 0.013206896372139454\n",
            "Train step - Step 1770, Loss 0.01261878665536642\n",
            "Train step - Step 1780, Loss 0.021335450932383537\n",
            "Train step - Step 1790, Loss 0.022157207131385803\n",
            "Train epoch - Accuracy: 0.9296969696969697 Loss: 0.019128949817533446 Corrects: 4602\n",
            "Starting epoch 47/70, LR = [2]\n",
            "Train step - Step 1800, Loss 0.031692780554294586\n",
            "Train step - Step 1810, Loss 0.019119825214147568\n",
            "Train step - Step 1820, Loss 0.019082441926002502\n",
            "Train step - Step 1830, Loss 0.014922164380550385\n",
            "Train epoch - Accuracy: 0.9294949494949495 Loss: 0.018774732528912902 Corrects: 4601\n",
            "Starting epoch 48/70, LR = [2]\n",
            "Train step - Step 1840, Loss 0.019599104300141335\n",
            "Train step - Step 1850, Loss 0.01540223229676485\n",
            "Train step - Step 1860, Loss 0.01756312884390354\n",
            "Train step - Step 1870, Loss 0.013894793577492237\n",
            "Train epoch - Accuracy: 0.9454545454545454 Loss: 0.015882106068910974 Corrects: 4680\n",
            "Starting epoch 49/70, LR = [2]\n",
            "Train step - Step 1880, Loss 0.02072114311158657\n",
            "Train step - Step 1890, Loss 0.014116043224930763\n",
            "Train step - Step 1900, Loss 0.014504400081932545\n",
            "Train step - Step 1910, Loss 0.019072633236646652\n",
            "Train epoch - Accuracy: 0.9412121212121212 Loss: 0.016150918261270332 Corrects: 4659\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "Train step - Step 1920, Loss 0.007144921459257603\n",
            "Train step - Step 1930, Loss 0.010303214192390442\n",
            "Train step - Step 1940, Loss 0.008524809964001179\n",
            "Train epoch - Accuracy: 0.9668686868686869 Loss: 0.01036530005435149 Corrects: 4786\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "Train step - Step 1950, Loss 0.007571057416498661\n",
            "Train step - Step 1960, Loss 0.009428384713828564\n",
            "Train step - Step 1970, Loss 0.005408479832112789\n",
            "Train step - Step 1980, Loss 0.004439841490238905\n",
            "Train epoch - Accuracy: 0.983030303030303 Loss: 0.006385402106915158 Corrects: 4866\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "Train step - Step 1990, Loss 0.005023502744734287\n",
            "Train step - Step 2000, Loss 0.004568540956825018\n",
            "Train step - Step 2010, Loss 0.0070972442626953125\n",
            "Train step - Step 2020, Loss 0.0053298594430089\n",
            "Train epoch - Accuracy: 0.984040404040404 Loss: 0.005717687456953255 Corrects: 4871\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "Train step - Step 2030, Loss 0.0033463460858911276\n",
            "Train step - Step 2040, Loss 0.0038331784307956696\n",
            "Train step - Step 2050, Loss 0.005004758946597576\n",
            "Train step - Step 2060, Loss 0.007339247968047857\n",
            "Train epoch - Accuracy: 0.9866666666666667 Loss: 0.00519399248054157 Corrects: 4884\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "Train step - Step 2070, Loss 0.003579548094421625\n",
            "Train step - Step 2080, Loss 0.002935364842414856\n",
            "Train step - Step 2090, Loss 0.002872584154829383\n",
            "Train step - Step 2100, Loss 0.004433474503457546\n",
            "Train epoch - Accuracy: 0.9882828282828283 Loss: 0.004428266563612704 Corrects: 4892\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "Train step - Step 2110, Loss 0.0029129565227776766\n",
            "Train step - Step 2120, Loss 0.0036446319427341223\n",
            "Train step - Step 2130, Loss 0.0039208801463246346\n",
            "Train step - Step 2140, Loss 0.003981394227594137\n",
            "Train epoch - Accuracy: 0.9929292929292929 Loss: 0.0037604088278844804 Corrects: 4915\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "Train step - Step 2150, Loss 0.0043821195140480995\n",
            "Train step - Step 2160, Loss 0.0022118918132036924\n",
            "Train step - Step 2170, Loss 0.0018221199279651046\n",
            "Train step - Step 2180, Loss 0.0035167147871106863\n",
            "Train epoch - Accuracy: 0.9919191919191919 Loss: 0.0036054623943537175 Corrects: 4910\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "Train step - Step 2190, Loss 0.001893431763164699\n",
            "Train step - Step 2200, Loss 0.0036885652225464582\n",
            "Train step - Step 2210, Loss 0.005160112865269184\n",
            "Train step - Step 2220, Loss 0.004473348613828421\n",
            "Train epoch - Accuracy: 0.9905050505050506 Loss: 0.0036415015923028644 Corrects: 4903\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "Train step - Step 2230, Loss 0.0044736131094396114\n",
            "Train step - Step 2240, Loss 0.0025921387132257223\n",
            "Train step - Step 2250, Loss 0.003662293078377843\n",
            "Train step - Step 2260, Loss 0.005879699252545834\n",
            "Train epoch - Accuracy: 0.9901010101010101 Loss: 0.0037449603162781155 Corrects: 4901\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "Train step - Step 2270, Loss 0.0012657821644097567\n",
            "Train step - Step 2280, Loss 0.001316371955908835\n",
            "Train step - Step 2290, Loss 0.003329036058858037\n",
            "Train step - Step 2300, Loss 0.003460868028923869\n",
            "Train epoch - Accuracy: 0.9941414141414141 Loss: 0.003033849928960806 Corrects: 4921\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "Train step - Step 2310, Loss 0.0013328735949471593\n",
            "Train step - Step 2320, Loss 0.0026733193080872297\n",
            "Train step - Step 2330, Loss 0.0051229060627520084\n",
            "Train epoch - Accuracy: 0.9943434343434343 Loss: 0.0026684027440808337 Corrects: 4922\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "Train step - Step 2340, Loss 0.0029888607095927\n",
            "Train step - Step 2350, Loss 0.0015394871588796377\n",
            "Train step - Step 2360, Loss 0.002500245114788413\n",
            "Train step - Step 2370, Loss 0.0022198427468538284\n",
            "Train epoch - Accuracy: 0.9943434343434343 Loss: 0.002564854943334605 Corrects: 4922\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "Train step - Step 2380, Loss 0.003391105681657791\n",
            "Train step - Step 2390, Loss 0.0037978186737746\n",
            "Train step - Step 2400, Loss 0.0022070412524044514\n",
            "Train step - Step 2410, Loss 0.002585943089798093\n",
            "Train epoch - Accuracy: 0.996969696969697 Loss: 0.002392006976191293 Corrects: 4935\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "Train step - Step 2420, Loss 0.002894174074754119\n",
            "Train step - Step 2430, Loss 0.0017946325242519379\n",
            "Train step - Step 2440, Loss 0.002098231576383114\n",
            "Train step - Step 2450, Loss 0.001963522983714938\n",
            "Train epoch - Accuracy: 0.9953535353535353 Loss: 0.002439738500185988 Corrects: 4927\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "Train step - Step 2460, Loss 0.0028753788210451603\n",
            "Train step - Step 2470, Loss 0.0014130882918834686\n",
            "Train step - Step 2480, Loss 0.004376239608973265\n",
            "Train step - Step 2490, Loss 0.001963683171197772\n",
            "Train epoch - Accuracy: 0.9957575757575757 Loss: 0.0022935657305737035 Corrects: 4929\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2500, Loss 0.0024064581375569105\n",
            "Train step - Step 2510, Loss 0.001980844186618924\n",
            "Train step - Step 2520, Loss 0.0016502684447914362\n",
            "Train step - Step 2530, Loss 0.0032346968073397875\n",
            "Train epoch - Accuracy: 0.9971717171717172 Loss: 0.001882455757245271 Corrects: 4936\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2540, Loss 0.002959401113912463\n",
            "Train step - Step 2550, Loss 0.0010753193637356162\n",
            "Train step - Step 2560, Loss 0.002499205991625786\n",
            "Train step - Step 2570, Loss 0.0024491033982485533\n",
            "Train epoch - Accuracy: 0.9973737373737374 Loss: 0.001995287639220631 Corrects: 4937\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2580, Loss 0.002801759634166956\n",
            "Train step - Step 2590, Loss 0.0026049779262393713\n",
            "Train step - Step 2600, Loss 0.0011786211980506778\n",
            "Train step - Step 2610, Loss 0.0033025105949491262\n",
            "Train epoch - Accuracy: 0.9965656565656565 Loss: 0.0019479888395378083 Corrects: 4933\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2620, Loss 0.0027153317350894213\n",
            "Train step - Step 2630, Loss 0.0023463231045752764\n",
            "Train step - Step 2640, Loss 0.001080017420463264\n",
            "Train step - Step 2650, Loss 0.0032948607113212347\n",
            "Train epoch - Accuracy: 0.9963636363636363 Loss: 0.0021217661245603752 Corrects: 4932\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2660, Loss 0.0014387598494067788\n",
            "Train step - Step 2670, Loss 0.004156345035880804\n",
            "Train step - Step 2680, Loss 0.001394410035572946\n",
            "Train step - Step 2690, Loss 0.0013046107487753034\n",
            "Train epoch - Accuracy: 0.9981818181818182 Loss: 0.001684935477637507 Corrects: 4941\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "Train step - Step 2700, Loss 0.0020945563446730375\n",
            "Train step - Step 2710, Loss 0.0012938595609739423\n",
            "Train step - Step 2720, Loss 0.0015032448573037982\n",
            "Train epoch - Accuracy: 0.9977777777777778 Loss: 0.0017020867887244681 Corrects: 4939\n",
            "Training finished in 406.2847566604614 seconds\n",
            "EVALUATION:  0.92 0.023757152259349823\n",
            "TEST GROUP:  0.884\n",
            "TEST ALL:  0.442\n",
            "GROUP:  3\n",
            "Starting epoch 1/70, LR = [2]\n",
            "Train step - Step 0, Loss 0.35266298055648804\n",
            "Train step - Step 10, Loss 0.12212328612804413\n",
            "Train step - Step 20, Loss 0.08218219131231308\n",
            "Train step - Step 30, Loss 0.07040272653102875\n",
            "Train epoch - Accuracy: 0.35818181818181816 Loss: 0.10919687200977345 Corrects: 1773\n",
            "Starting epoch 2/70, LR = [2]\n",
            "Train step - Step 40, Loss 0.06564679741859436\n",
            "Train step - Step 50, Loss 0.0592169389128685\n",
            "Train step - Step 60, Loss 0.0518423430621624\n",
            "Train step - Step 70, Loss 0.05591166764497757\n",
            "Train epoch - Accuracy: 0.6513131313131313 Loss: 0.05490660672687521 Corrects: 3224\n",
            "Starting epoch 3/70, LR = [2]\n",
            "Train step - Step 80, Loss 0.04804922640323639\n",
            "Train step - Step 90, Loss 0.04783447086811066\n",
            "Train step - Step 100, Loss 0.04122575744986534\n",
            "Train step - Step 110, Loss 0.03808792307972908\n",
            "Train epoch - Accuracy: 0.7426262626262626 Loss: 0.04342038382333938 Corrects: 3676\n",
            "Starting epoch 4/70, LR = [2]\n",
            "Train step - Step 120, Loss 0.037509821355342865\n",
            "Train step - Step 130, Loss 0.03703051432967186\n",
            "Train step - Step 140, Loss 0.03029746375977993\n",
            "Train step - Step 150, Loss 0.04941616207361221\n",
            "Train epoch - Accuracy: 0.7876767676767676 Loss: 0.03622016331613666 Corrects: 3899\n",
            "Starting epoch 5/70, LR = [2]\n",
            "Train step - Step 160, Loss 0.03648333624005318\n",
            "Train step - Step 170, Loss 0.03561161831021309\n",
            "Train step - Step 180, Loss 0.032545123249292374\n",
            "Train step - Step 190, Loss 0.03272250294685364\n",
            "Train epoch - Accuracy: 0.8105050505050505 Loss: 0.03212938264840179 Corrects: 4012\n",
            "Starting epoch 6/70, LR = [2]\n",
            "Train step - Step 200, Loss 0.030260203406214714\n",
            "Train step - Step 210, Loss 0.03359047695994377\n",
            "Train step - Step 220, Loss 0.037933915853500366\n",
            "Train step - Step 230, Loss 0.03176664188504219\n",
            "Train epoch - Accuracy: 0.8280808080808081 Loss: 0.029942112707278944 Corrects: 4099\n",
            "Starting epoch 7/70, LR = [2]\n",
            "Train step - Step 240, Loss 0.03060873970389366\n",
            "Train step - Step 250, Loss 0.025081882253289223\n",
            "Train step - Step 260, Loss 0.02231759764254093\n",
            "Train step - Step 270, Loss 0.02236504666507244\n",
            "Train epoch - Accuracy: 0.8448484848484848 Loss: 0.02708846489287386 Corrects: 4182\n",
            "Starting epoch 8/70, LR = [2]\n",
            "Train step - Step 280, Loss 0.02423119731247425\n",
            "Train step - Step 290, Loss 0.020075997337698936\n",
            "Train step - Step 300, Loss 0.021541137248277664\n",
            "Train step - Step 310, Loss 0.020967837423086166\n",
            "Train epoch - Accuracy: 0.8642424242424243 Loss: 0.024506078862180612 Corrects: 4278\n",
            "Starting epoch 9/70, LR = [2]\n",
            "Train step - Step 320, Loss 0.021173721179366112\n",
            "Train step - Step 330, Loss 0.018949974328279495\n",
            "Train step - Step 340, Loss 0.028835011646151543\n",
            "Train step - Step 350, Loss 0.032685790210962296\n",
            "Train epoch - Accuracy: 0.8652525252525253 Loss: 0.022950992179338377 Corrects: 4283\n",
            "Starting epoch 10/70, LR = [2]\n",
            "Train step - Step 360, Loss 0.020213162526488304\n",
            "Train step - Step 370, Loss 0.02006094716489315\n",
            "Train step - Step 380, Loss 0.022259673103690147\n",
            "Train epoch - Accuracy: 0.8747474747474747 Loss: 0.02198729294719118 Corrects: 4330\n",
            "Starting epoch 11/70, LR = [2]\n",
            "Train step - Step 390, Loss 0.020294861868023872\n",
            "Train step - Step 400, Loss 0.020464468747377396\n",
            "Train step - Step 410, Loss 0.018385086208581924\n",
            "Train step - Step 420, Loss 0.01807243935763836\n",
            "Train epoch - Accuracy: 0.8745454545454545 Loss: 0.021537973978603728 Corrects: 4329\n",
            "Starting epoch 12/70, LR = [2]\n",
            "Train step - Step 430, Loss 0.014602860435843468\n",
            "Train step - Step 440, Loss 0.01844589039683342\n",
            "Train step - Step 450, Loss 0.021584197878837585\n",
            "Train step - Step 460, Loss 0.021073073148727417\n",
            "Train epoch - Accuracy: 0.8846464646464647 Loss: 0.02039071719002242 Corrects: 4379\n",
            "Starting epoch 13/70, LR = [2]\n",
            "Train step - Step 470, Loss 0.021487517282366753\n",
            "Train step - Step 480, Loss 0.01739770732820034\n",
            "Train step - Step 490, Loss 0.022984422743320465\n",
            "Train step - Step 500, Loss 0.02001182734966278\n",
            "Train epoch - Accuracy: 0.8905050505050505 Loss: 0.019696995958384842 Corrects: 4408\n",
            "Starting epoch 14/70, LR = [2]\n",
            "Train step - Step 510, Loss 0.01700243167579174\n",
            "Train step - Step 520, Loss 0.02380007691681385\n",
            "Train step - Step 530, Loss 0.02227056585252285\n",
            "Train step - Step 540, Loss 0.01624714583158493\n",
            "Train epoch - Accuracy: 0.8933333333333333 Loss: 0.019039836701269103 Corrects: 4422\n",
            "Starting epoch 15/70, LR = [2]\n",
            "Train step - Step 550, Loss 0.019841890782117844\n",
            "Train step - Step 560, Loss 0.01965864934027195\n",
            "Train step - Step 570, Loss 0.014396032318472862\n",
            "Train step - Step 580, Loss 0.02021968550980091\n",
            "Train epoch - Accuracy: 0.901010101010101 Loss: 0.017772986320684653 Corrects: 4460\n",
            "Starting epoch 16/70, LR = [2]\n",
            "Train step - Step 590, Loss 0.01409969199448824\n",
            "Train step - Step 600, Loss 0.014153556898236275\n",
            "Train step - Step 610, Loss 0.016979459673166275\n",
            "Train step - Step 620, Loss 0.016436735168099403\n",
            "Train epoch - Accuracy: 0.9088888888888889 Loss: 0.016382058798664747 Corrects: 4499\n",
            "Starting epoch 17/70, LR = [2]\n",
            "Train step - Step 630, Loss 0.020800288766622543\n",
            "Train step - Step 640, Loss 0.015290242619812489\n",
            "Train step - Step 650, Loss 0.01986781693994999\n",
            "Train step - Step 660, Loss 0.02046799845993519\n",
            "Train epoch - Accuracy: 0.9076767676767676 Loss: 0.016731816510284186 Corrects: 4493\n",
            "Starting epoch 18/70, LR = [2]\n",
            "Train step - Step 670, Loss 0.02059890329837799\n",
            "Train step - Step 680, Loss 0.01693188026547432\n",
            "Train step - Step 690, Loss 0.013517694547772408\n",
            "Train step - Step 700, Loss 0.015347390435636044\n",
            "Train epoch - Accuracy: 0.9113131313131313 Loss: 0.015572423298250546 Corrects: 4511\n",
            "Starting epoch 19/70, LR = [2]\n",
            "Train step - Step 710, Loss 0.008227002806961536\n",
            "Train step - Step 720, Loss 0.013987197540700436\n",
            "Train step - Step 730, Loss 0.011739223264157772\n",
            "Train step - Step 740, Loss 0.01740328222513199\n",
            "Train epoch - Accuracy: 0.9195959595959596 Loss: 0.01481043155446197 Corrects: 4552\n",
            "Starting epoch 20/70, LR = [2]\n",
            "Train step - Step 750, Loss 0.01550209242850542\n",
            "Train step - Step 760, Loss 0.01163950189948082\n",
            "Train step - Step 770, Loss 0.017101328819990158\n",
            "Train epoch - Accuracy: 0.9191919191919192 Loss: 0.014932385819277378 Corrects: 4550\n",
            "Starting epoch 21/70, LR = [2]\n",
            "Train step - Step 780, Loss 0.012696463614702225\n",
            "Train step - Step 790, Loss 0.010404464788734913\n",
            "Train step - Step 800, Loss 0.011964384466409683\n",
            "Train step - Step 810, Loss 0.015415259636938572\n",
            "Train epoch - Accuracy: 0.9301010101010101 Loss: 0.01284050268506763 Corrects: 4604\n",
            "Starting epoch 22/70, LR = [2]\n",
            "Train step - Step 820, Loss 0.011438518762588501\n",
            "Train step - Step 830, Loss 0.012063213624060154\n",
            "Train step - Step 840, Loss 0.014638977125287056\n",
            "Train step - Step 850, Loss 0.018151331692934036\n",
            "Train epoch - Accuracy: 0.9321212121212121 Loss: 0.01283667635661785 Corrects: 4614\n",
            "Starting epoch 23/70, LR = [2]\n",
            "Train step - Step 860, Loss 0.014357252046465874\n",
            "Train step - Step 870, Loss 0.011064925231039524\n",
            "Train step - Step 880, Loss 0.019384408369660378\n",
            "Train step - Step 890, Loss 0.007490999065339565\n",
            "Train epoch - Accuracy: 0.9288888888888889 Loss: 0.013742038333656812 Corrects: 4598\n",
            "Starting epoch 24/70, LR = [2]\n",
            "Train step - Step 900, Loss 0.01562267355620861\n",
            "Train step - Step 910, Loss 0.010020052082836628\n",
            "Train step - Step 920, Loss 0.010835394263267517\n",
            "Train step - Step 930, Loss 0.008910988457500935\n",
            "Train epoch - Accuracy: 0.9301010101010101 Loss: 0.013006130570564607 Corrects: 4604\n",
            "Starting epoch 25/70, LR = [2]\n",
            "Train step - Step 940, Loss 0.01499682106077671\n",
            "Train step - Step 950, Loss 0.006663089152425528\n",
            "Train step - Step 960, Loss 0.011399895884096622\n",
            "Train step - Step 970, Loss 0.007453154306858778\n",
            "Train epoch - Accuracy: 0.9266666666666666 Loss: 0.01277846399468906 Corrects: 4587\n",
            "Starting epoch 26/70, LR = [2]\n",
            "Train step - Step 980, Loss 0.01620747707784176\n",
            "Train step - Step 990, Loss 0.012882948853075504\n",
            "Train step - Step 1000, Loss 0.013795817270874977\n",
            "Train step - Step 1010, Loss 0.015836073085665703\n",
            "Train epoch - Accuracy: 0.9385858585858586 Loss: 0.011837359993445753 Corrects: 4646\n",
            "Starting epoch 27/70, LR = [2]\n",
            "Train step - Step 1020, Loss 0.012661302462220192\n",
            "Train step - Step 1030, Loss 0.013809850439429283\n",
            "Train step - Step 1040, Loss 0.009237089194357395\n",
            "Train step - Step 1050, Loss 0.013796067796647549\n",
            "Train epoch - Accuracy: 0.9406060606060606 Loss: 0.011408511306526084 Corrects: 4656\n",
            "Starting epoch 28/70, LR = [2]\n",
            "Train step - Step 1060, Loss 0.012041084468364716\n",
            "Train step - Step 1070, Loss 0.00811582151800394\n",
            "Train step - Step 1080, Loss 0.01019228994846344\n",
            "Train step - Step 1090, Loss 0.016221310943365097\n",
            "Train epoch - Accuracy: 0.942020202020202 Loss: 0.010931335571739408 Corrects: 4663\n",
            "Starting epoch 29/70, LR = [2]\n",
            "Train step - Step 1100, Loss 0.006186176557093859\n",
            "Train step - Step 1110, Loss 0.012281359173357487\n",
            "Train step - Step 1120, Loss 0.006037046667188406\n",
            "Train step - Step 1130, Loss 0.015260246582329273\n",
            "Train epoch - Accuracy: 0.9426262626262626 Loss: 0.010445335982649614 Corrects: 4666\n",
            "Starting epoch 30/70, LR = [2]\n",
            "Train step - Step 1140, Loss 0.008890144526958466\n",
            "Train step - Step 1150, Loss 0.010226527228951454\n",
            "Train step - Step 1160, Loss 0.010695036500692368\n",
            "Train epoch - Accuracy: 0.9385858585858586 Loss: 0.011627610451494805 Corrects: 4646\n",
            "Starting epoch 31/70, LR = [2]\n",
            "Train step - Step 1170, Loss 0.015758242458105087\n",
            "Train step - Step 1180, Loss 0.012697629630565643\n",
            "Train step - Step 1190, Loss 0.007279234007000923\n",
            "Train step - Step 1200, Loss 0.009643597528338432\n",
            "Train epoch - Accuracy: 0.9474747474747475 Loss: 0.009997457395298313 Corrects: 4690\n",
            "Starting epoch 32/70, LR = [2]\n",
            "Train step - Step 1210, Loss 0.006373121868818998\n",
            "Train step - Step 1220, Loss 0.00624897750094533\n",
            "Train step - Step 1230, Loss 0.009837244637310505\n",
            "Train step - Step 1240, Loss 0.012095221318304539\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.009832649998216316 Corrects: 4703\n",
            "Starting epoch 33/70, LR = [2]\n",
            "Train step - Step 1250, Loss 0.00881249364465475\n",
            "Train step - Step 1260, Loss 0.007874124683439732\n",
            "Train step - Step 1270, Loss 0.015137188136577606\n",
            "Train step - Step 1280, Loss 0.010692795738577843\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.009112436044396776 Corrects: 4703\n",
            "Starting epoch 34/70, LR = [2]\n",
            "Train step - Step 1290, Loss 0.005640859715640545\n",
            "Train step - Step 1300, Loss 0.010081232525408268\n",
            "Train step - Step 1310, Loss 0.014683092944324017\n",
            "Train step - Step 1320, Loss 0.012466007843613625\n",
            "Train epoch - Accuracy: 0.9406060606060606 Loss: 0.011610343910452693 Corrects: 4656\n",
            "Starting epoch 35/70, LR = [2]\n",
            "Train step - Step 1330, Loss 0.0059059374034404755\n",
            "Train step - Step 1340, Loss 0.0142081119120121\n",
            "Train step - Step 1350, Loss 0.014881717041134834\n",
            "Train step - Step 1360, Loss 0.008837450295686722\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.009729343055926189 Corrects: 4703\n",
            "Starting epoch 36/70, LR = [2]\n",
            "Train step - Step 1370, Loss 0.010101868771016598\n",
            "Train step - Step 1380, Loss 0.0095260264351964\n",
            "Train step - Step 1390, Loss 0.007985921576619148\n",
            "Train step - Step 1400, Loss 0.009614543989300728\n",
            "Train epoch - Accuracy: 0.9490909090909091 Loss: 0.009451713057027923 Corrects: 4698\n",
            "Starting epoch 37/70, LR = [2]\n",
            "Train step - Step 1410, Loss 0.010480918921530247\n",
            "Train step - Step 1420, Loss 0.010994554497301579\n",
            "Train step - Step 1430, Loss 0.013562602922320366\n",
            "Train step - Step 1440, Loss 0.006867910269647837\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.009282611110294709 Corrects: 4703\n",
            "Starting epoch 38/70, LR = [2]\n",
            "Train step - Step 1450, Loss 0.0072119031101465225\n",
            "Train step - Step 1460, Loss 0.010519197210669518\n",
            "Train step - Step 1470, Loss 0.010924878530204296\n",
            "Train step - Step 1480, Loss 0.010122427716851234\n",
            "Train epoch - Accuracy: 0.9397979797979797 Loss: 0.01105400192052728 Corrects: 4652\n",
            "Starting epoch 39/70, LR = [2]\n",
            "Train step - Step 1490, Loss 0.012259192764759064\n",
            "Train step - Step 1500, Loss 0.021076878532767296\n",
            "Train step - Step 1510, Loss 0.009098595939576626\n",
            "Train step - Step 1520, Loss 0.017000209540128708\n",
            "Train epoch - Accuracy: 0.9470707070707071 Loss: 0.010444669640726514 Corrects: 4688\n",
            "Starting epoch 40/70, LR = [2]\n",
            "Train step - Step 1530, Loss 0.010035817511379719\n",
            "Train step - Step 1540, Loss 0.008298467844724655\n",
            "Train step - Step 1550, Loss 0.010190248489379883\n",
            "Train epoch - Accuracy: 0.9535353535353536 Loss: 0.008598944381105178 Corrects: 4720\n",
            "Starting epoch 41/70, LR = [2]\n",
            "Train step - Step 1560, Loss 0.005083548370748758\n",
            "Train step - Step 1570, Loss 0.007784984540194273\n",
            "Train step - Step 1580, Loss 0.008403550833463669\n",
            "Train step - Step 1590, Loss 0.013891680166125298\n",
            "Train epoch - Accuracy: 0.9561616161616162 Loss: 0.008280981104407045 Corrects: 4733\n",
            "Starting epoch 42/70, LR = [2]\n",
            "Train step - Step 1600, Loss 0.006689437665045261\n",
            "Train step - Step 1610, Loss 0.008135640993714333\n",
            "Train step - Step 1620, Loss 0.010825779289007187\n",
            "Train step - Step 1630, Loss 0.004959254991263151\n",
            "Train epoch - Accuracy: 0.9612121212121212 Loss: 0.007952388445987845 Corrects: 4758\n",
            "Starting epoch 43/70, LR = [2]\n",
            "Train step - Step 1640, Loss 0.00562081066891551\n",
            "Train step - Step 1650, Loss 0.00694376090541482\n",
            "Train step - Step 1660, Loss 0.00703728711232543\n",
            "Train step - Step 1670, Loss 0.005459585692733526\n",
            "Train epoch - Accuracy: 0.955959595959596 Loss: 0.008616107691719074 Corrects: 4732\n",
            "Starting epoch 44/70, LR = [2]\n",
            "Train step - Step 1680, Loss 0.012139140628278255\n",
            "Train step - Step 1690, Loss 0.009411933831870556\n",
            "Train step - Step 1700, Loss 0.0070087723433971405\n",
            "Train step - Step 1710, Loss 0.007114705629646778\n",
            "Train epoch - Accuracy: 0.9547474747474748 Loss: 0.008777330520967342 Corrects: 4726\n",
            "Starting epoch 45/70, LR = [2]\n",
            "Train step - Step 1720, Loss 0.0064211501739919186\n",
            "Train step - Step 1730, Loss 0.005560960620641708\n",
            "Train step - Step 1740, Loss 0.0033696256577968597\n",
            "Train step - Step 1750, Loss 0.010398042388260365\n",
            "Train epoch - Accuracy: 0.9595959595959596 Loss: 0.007876360344916884 Corrects: 4750\n",
            "Starting epoch 46/70, LR = [2]\n",
            "Train step - Step 1760, Loss 0.006306668743491173\n",
            "Train step - Step 1770, Loss 0.009662346914410591\n",
            "Train step - Step 1780, Loss 0.0027505457401275635\n",
            "Train step - Step 1790, Loss 0.007156122010201216\n",
            "Train epoch - Accuracy: 0.9624242424242424 Loss: 0.007432514855619332 Corrects: 4764\n",
            "Starting epoch 47/70, LR = [2]\n",
            "Train step - Step 1800, Loss 0.004130183253437281\n",
            "Train step - Step 1810, Loss 0.0030859760008752346\n",
            "Train step - Step 1820, Loss 0.0031533038709312677\n",
            "Train step - Step 1830, Loss 0.005821757949888706\n",
            "Train epoch - Accuracy: 0.9684848484848485 Loss: 0.006615401048100356 Corrects: 4794\n",
            "Starting epoch 48/70, LR = [2]\n",
            "Train step - Step 1840, Loss 0.004940843675285578\n",
            "Train step - Step 1850, Loss 0.008420541882514954\n",
            "Train step - Step 1860, Loss 0.005836762487888336\n",
            "Train step - Step 1870, Loss 0.004992457572370768\n",
            "Train epoch - Accuracy: 0.9644444444444444 Loss: 0.007306149196669911 Corrects: 4774\n",
            "Starting epoch 49/70, LR = [2]\n",
            "Train step - Step 1880, Loss 0.010786527767777443\n",
            "Train step - Step 1890, Loss 0.006471360567957163\n",
            "Train step - Step 1900, Loss 0.006134164985269308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BCQoMhtWDJH"
      },
      "source": [
        "**Results fine tuning (catastrophic learning)**<br>\n",
        "What we expect is a dramatic drop in the perfomances with repsect to the Joint Training and the incapacity to learn new things without forgetting the old ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_4xLfwcpDz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "ef78d58c-5bd4-46df-9adb-27b5def3dc88"
      },
      "source": [
        "method = \"finetuning\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "utils.plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "utils.plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "utils.writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABI8AAAG5CAYAAAAH7hQVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de7itZV0v/O9PlgSoeYilKaDYFg/oTlQiTbeaWIEHyEMK5ancorswbbsrq73ZyPu2387mW5Thzq15QqA0NPIQmpYpcfDEQQoNBURFBVFIOfjbf4xn6WQ677XmRMYac671+VzXvNZ4nnHPZ3zHeNbiWuvLfd+zujsAAAAAsJJbLToAAAAAAOuX8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgHATqCqfqaq3nULXevvq+o/3xLXYj6q6ryqevQqx3ZV3WvOkQCADUx5BAA7iKp6RFX9U1V9paq+XFUfqKofSpLufkN3//g6yLhrVR1TVRdW1TVVdVlV/W1VLTxbklTV/avqXdPnd1VVnV1Vj1t0rq2pqtdU1f+79Fx337+7//4Wfo0bququt9Q1AYCNQ3kEADuAqvreJG9P8kdJ7pRkryQvS/KNReZawSlJDk/yrCR3THLPJK9I8viVBlfVpu0XLUnytiTvTvL9Se6c5BeTXL2dM6wrVXWbJE9J8pUkz9jOr7297z8AsALlEQDsGO6dJN39pu6+sbv/vbvf1d0fS5Kqek5V/eOWwdNSpRdU1b9OM2yOr6qantulqn6/qr5YVf9WVUdP41f8h3xV/VxVXVBVV1bVO6vqHoNxj03yY0kO7+4zuvu66esd3f2iJeMurqpfraqPJbmmqjZV1WHTUqyrpmVz91v2Xu615PhbM3Gq6tFVdWlV/fr0fi6uqp8Z5NszszLrVUuyfaC7l35uT6iqj0w5/qmqfnDJcw+qqnOq6qtV9eaqOnFJjpt8/stzV9X3VNXvVdVnqurzVfXKqtp92Xt4SVV9oaour6qfnZ47KsnPJPmVqvpaVb1tyWf42OnxQVX1wSnz5VX1x1W160qfwcBTklyV5Lgkz172Hu5UVf+nqj473f+3Lnnu8OmzurqqPllVhyzPNh0fW1Wvnx7vO30uz62qzyR5z3T+5Kr63DSr7v1Vdf8l37/79Pv109Pz/zid+5uqeuGyvB+rqiet4b0DAFEeAcCO4l+S3FhVr62qQ6vqjqv4nick+aEkP5jkaUl+Yjr/vCSHJjkgyYOT/OToAlV1eJJfT/LkJJuT/EOSNw2GPzbJGd196SqyHZnZbKQ7JPmB6Zovnl7jtCRvW0MB8v1J9sxsNtazk5xQVfdZYdyXklyU5PVV9ZNVdZelT1bVg5K8Osnzk3xfkj9LcupU/Oya5K1JXpfZzK+TMytdVuu3MisAD0hyrynrMcvew+2n889NcnxV3bG7T0jyhiS/09237e4nrnDtG5P80vQZPCzJwUl+fg3Znp3Z539ikvtW1UOWPPe6JHskuX9mM7VenswKqyR/keSXM7uHj0xy8Rpe81FJ7pdv/5782yT7Ta9xTmbveYvfS/KQJD+S2Wf/K0m+meS1WTJTqqoemNnn9zdryAEARHkEADuE7r46ySOSdJJXJbmiqk5dXoAs81vdfVV3fybJezMrLpJZkfSK7r60u6/MrNgYeUGS/6+7L+juG5L8ryQHDGYf7Znkc1sOplkrV02zRb6+bOz/392XdPe/J3l6kr/p7nd39/WZlQW7Z1YWrNb/6O5vdPf7MisPnrZ8QHd3kh/NrOT4/SSXT7Nc9puGHJXkz6ZZUzd292szWxb40Onr1kn+sLuv7+5Tkpy5mmBVVdO1f6m7v9zdX83sczxiybDrkxw3Xfu0JF9LslIB9h26++zu/lB339DdF2dWej1qldnuntln8sbu/nyS0zNbcpia7X90aJIXdPeVU7b3Td/63CSvnu7ZN7v7su7+xGpec3Jsd18z3f9096u7+6vd/Y0kxyZ5YFXdvqpuleTnkrxoeo0bu/ufpnGnJrn3kvv3zCRv7u7r1pADAIjyCAB2GFOB85zu3jvJA5LcLckfbuVbPrfk8bVJbjs9vluSS5Y8t/TxcvdI8oqpBLoqyZeTVGYzPJb7UpJvbbg8FSV3yGzWyPcsG7v0Ne+W5NNLvu+b0/MrvcZKruzua5Ycf3q65neYCrOju/s/TO/tmsxm0GQ6fsmW9zq9332ma90tyWVTAbX0dVZjc2azd85ect13TOe3+NJUzm2x9H5tVVXdu6rePi37ujqzYmrPVWZ7ZpILuvsj0/Ebkvx0Vd06s/f+5algXG6fJJ9c5Wus5Fv3v2bLKH9rWvp2db49g2nP6Wu3lV6ru7+e5M1JnjGVTEdmNlMKAFgj5REA7ICmWR6vyaxEWqvLk+y95HifrYy9JMnzu/sOS7527+5/WmHs6Ul+qKr2XuG55ZaWMJ/NrLhJ8q2ZOvskuWw6dW1m5csW37/sWnes2abPW9x9uubWA3RfkuT4fPszvCTJby57r3t095sy+8z2mrItfZ0trlmasaqWZvxikn9Pcv8l1719d6+qHMpNP6uV/GmSTyTZr7u/N7NlhrX1b/mWZyX5gal4+lySP8issHlcZp/HnarqDit83yVJ/sPgmjf5LPKd9yu56Xv66cw2WX9sZkv39p3OV2af3de38lqvzWxPqIOTXNvdHxyMAwC2QnkEADuAqrrvtKHy3tPxPpnNtPjQzbjcSUleVFV7TcXAr25l7CuT/NqWDYynpUQ/tdLA7n5XZsvj3lpVP1xVu04zWB66ijyPr6qDp/EvyWy52JaC6iOZzYbZZdqUeaUlWS+bXu8/ZbbX08nLB1TVHavqZVV1r6q6Vc020P65fPszfFWSF0zZq6puU1WPr6rbJflgkhuS/GJV3bqqnpzkoCWX/2iS+1fVAVW1W2ZLr7Z8Lt+crv3yqrrzlGWvqvqJrM7nM9sXauR2mf3EuK9V1X2T/JfVXLSqHpZZKXNQZksaD8isSHtjkmd19+WZ7UX0J9Nnd+uqeuT07X+e5Gene3ar6f3cd3ruI0mOmMYfmOSp24hyu8zu95cyK53+15Ynps/u1Un+oKruNv0eeFhVfc/0/Acz2//o92PWEQDcbMojANgxfDXJDyc5o6quyazwODezomWtXpXkXUk+luTDmW1QfUNmGy/fRHe/JclvJzlxWlJ0bmb74Iw8Kcnbk7w+s5/g9W+ZzQwZFiXdfWFmGx//UWYzTZ6Y5IlL9q550XTuqulab112ic8luTKz2UZvyGyPnpX237kus1ktf5dZ2XJuZqXFc6YcZ2W2mfgfT9e7aMlz12W2afhzMlu69/Qkf7XkPfxLZj+t7O+S/GuSm/zktcwKuouSfGj6HP8uq9zTKLOiZv9pydvy954k/y2z2TtfzezevnmV1312kr/u7o939+e2fCV5RZInVNWdMlvWdn1mM5u+kNmm5unuf07ys5ltoP2VJO/Lt2eP/Y/MSqkrk7wsszJqa/4isyWAlyU5P99ZiP63JB/PbI+pL2f2+/FWy77/P2b2ew4AuBnqpkvzAQBuqqoOTfLK7l5pE+x1raoeneT10z5Q2/u1X5Pk0u7+79v7tfm2qnpWkqO6+xGLzgIAG5WZRwDATVTV7lX1uKraVFV7JfmfSd6y6FywVlW1R5KfT3LCorMAwEamPAIAlqvMlhNdmdmytQuSHLPQRLBG055RV2S2J9S2lsYBAFth2RoAAAAAQ2YeAQAAADC0adEB1mrPPffsfffdd9ExAAAAAHYYZ5999he7e/NKz2248mjffffNWWedtegYAAAAADuMqvr06DnL1gAAAAAYUh4BAAAAMKQ8AgAAAGBIeQQAAADAkPIIAAAAgCHlEQAAAABDyiMAAAAAhpRHAAAAAAwpjwAAAAAYUh4BAAAAMKQ8AgAAAGBIeQQAAADAkPIIAAAAgCHlEQAAAABDyiMAAAAAhpRHAAAAAAxtWnQAAACARTn22GMXHWGn4HOGjc3MIwAAAACGlEcAAAAADFm2xrr1meP+46Ij7PDufszHFx0BAACAdc7MIwAAAACGzDwCAAAAtrsHnvLORUfY4X30qT9xi1zHzCMAAAAAhpRHAAAAAAxZtgYAsA785jOeuugIO4XfeP0pi44AABuOmUcAAAAADJl5BMBNvO+Rj1p0hB3eo97/vkVHAACAVVMeAQDAd+mC33zPoiPs8O73G49ZdATWoZNOPmjREXZ4T/upf150BNYBy9YAAAAAGFIeAQAAADCkPAIAAABgSHkEAAAAwJDyCAAAAIAh5REAAAAAQ5sWHQDY8Tz8jx6+6Ag7hQ+88AOLjgAAAOwEzDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADG1adAAA4Jbzxy9526Ij7PCO/v0nLjoCAMB2ZeYRAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMDQXMujqjqkqi6sqouq6qUrPH/3qnpvVX24qj5WVY+bZx4AAAAA1mZu5VFV7ZLk+CSHJtk/yZFVtf+yYf89yUnd/aAkRyT5k3nlAQAAAGDt5jnz6KAkF3X3p7r7uiQnJjl82ZhO8r3T49sn+ewc8wAAAACwRvMsj/ZKcsmS40unc0sdm+QZVXVpktOSvHClC1XVUVV1VlWddcUVV8wjKwAAAAArWPSG2UcmeU13753kcUleV1Xfkam7T+juA7v7wM2bN2/3kAAAAAA7q3mWR5cl2WfJ8d7TuaWem+SkJOnuDybZLcmec8wEAAAAwBrMszw6M8l+VXXPqto1sw2xT1025jNJDk6SqrpfZuWRdWkAAAAA68TcyqPuviHJ0UnemeSCzH6q2nlVdVxVHTYNe0mS51XVR5O8KclzurvnlQkAAACAtdk0z4t392mZbYS99NwxSx6fn+Th88wAAAAAwM236A2zAQAAAFjH5jrzaNEe8st/segIO4Wzf/dZi44AAAAAzImZRwAAAAAMKY8AAAAAGFIeAQAAADCkPAIAAABgSHkEAAAAwJDyCAAAAIAh5REAAAAAQ8ojAAAAAIaURwAAAAAMKY8AAAAAGFIeAQAAADCkPAIAAABgSHkEAAAAwJDyCAAAAIAh5REAAAAAQ8ojAAAAAIaURwAAAAAMKY8AAAAAGFIeAQAAADCkPAIAAABgSHkEAAAAwJDyCAAAAIAh5REAAAAAQ8ojAAAAAIaURwAAAAAMKY8AAAAAGFIeAQAAADCkPAIAAABgSHkEAAAAwJDyCAAAAIAh5REAAAAAQ8ojAAAAAIaURwAAAAAMKY8AAAAAGFIeAQAAADCkPAIAAABgSHkEAAAAwJDyCAAAAIAh5REAAAAAQ8ojAAAAAIaURwAAAAAMKY8AAAAAGFIeAQAAADCkPAIAAABgSHkEAAAAwJDyCAAAAIAh5REAAAAAQ8ojAAAAAIaURwAAAAAMKY8AAAAAGFIeAQAAADCkPAIAAABgSHkEAAAAwJDyCAAAAIAh5REAAAAAQ8ojAAAAAIaURwAAAAAMKY8AAAAAGFIeAQAAADCkPAIAAABgSHkEAAAAwJDyCAAAAIAh5REAAAAAQ8ojAAAAAIaURwAAAAAMKY8AAAAAGFIeAQAAADCkPAIAAABgSHkEAAAAwNBcy6OqOqSqLqyqi6rqpYMxT6uq86vqvKp64zzzAAAAALA2m+Z14araJcnxSX4syaVJzqyqU7v7/CVj9kvya0ke3t1XVtWd55UHAAAAgLWb58yjg5Jc1N2f6u7rkpyY5PBlY56X5PjuvjJJuvsLc8wDAAAAwBrNszzaK8klS44vnc4tde8k966qD1TVh6rqkJUuVFVHVdVZVXXWFVdcMae4AAAAACy36A2zNyXZL8mjkxyZ5FVVdYflg7r7hO4+sLsP3Lx583aOCAAAALDzmmd5dFmSfZYc7z2dW+rSJKd29/Xd/W9J/iWzMgkAAACAdWCe5dGZSfarqntW1a5Jjkhy6rIxb81s1lGqas/MlrF9ao6ZAAAAAFiDuZVH3X1DkqOTvDPJBUlO6u7zquq4qjpsGvbOJF+qqvOTvDfJL3f3l+aVCQAAAIC12TTPi3f3aUlOW3bumCWPO8l/nb4AAAAAWGcWvWE2AAAAAOuY8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABjaZnlUVU+sKiUTAAAAwE5oNaXQ05P8a1X9TlXdd96BAAAAAFg/tlkedfczkjwoySeTvKaqPlhVR1XV7eaeDgAAAICFWtVytO6+OskpSU5MctckT0pyTlW9cI7ZAAAAAFiw1ex5dFhVvSXJ3ye5dZKDuvvQJA9M8pL5xgMAAABgkTatYsxTkry8u9+/9GR3X1tVz51PLAAAAADWg9WUR8cmuXzLQVXtnuQu3X1xd58+r2AAAAAALN5q9jw6Ock3lxzfOJ0DAAAAYAe3mvJoU3dft+Vgerzr/CIBAAAAsF6spjy6oqoO23JQVYcn+eL8IgEAAACwXqxmz6MXJHlDVf1xkkpySZJnzTUVAAAAAOvCNsuj7v5kkodW1W2n46/NPRUAAAAA68JqZh6lqh6f5P5JdquqJEl3HzfHXAAAAACsA9vc86iqXpnk6UlemNmytZ9Kco855wIAAABgHVjNhtk/0t3PSnJld78sycOS3Hu+sQAAAABYD1ZTHn19+vXaqrpbkuuT3HV+kQAAAABYL1az59HbquoOSX43yTlJOsmr5poKAAAAgHVhq+VRVd0qyendfVWSv6yqtyfZrbu/sl3SAQAAALBQW1221t3fTHL8kuNvKI4AAAAAdh6r2fPo9Kp6SlXV3NMAAAAAsK6spjx6fpKTk3yjqq6uqq9W1dVzzgUAAADAOrDNDbO7+3bbIwgAAAAA6882y6OqeuRK57v7/bd8HAAAAADWk22WR0l+ecnj3ZIclOTsJI+ZSyIAAAAA1o3VLFt74tLjqtonyR/OLREAAAAA68ZqNsxe7tIk97ulgwAAAACw/qxmz6M/StLT4a2SHJDknHmGAgAAAGB9WM2eR2cteXxDkjd19wfmlAcAAACAdWQ15dEpSb7e3TcmSVXtUlV7dPe1840GAAAAwKKtZs+j05PsvuR49yR/N584AAAAAKwnqymPduvur205mB7vMb9IAAAAAKwXqymPrqmqB285qKqHJPn3+UUCAAAAYL1YzZ5HL05yclV9Nkkl+f4kT59rKgAAAADWhW2WR919ZlXdN8l9plMXdvf1840FAAAAwHqwzWVrVfULSW7T3ed297lJbltVPz//aAAAAAAs2mr2PHped1+15aC7r0zyvPlFAgAAAGC9WE15tEtV1ZaDqtolya7ziwQAAADAerGaDbPfkeTNVfVn0/Hzk/zt/CIBAAAAsF6spjz61SRHJXnBdPyxzH7iGgAAAAA7uG0uW+vubyY5I8nFSQ5K8pgkF8w3FgAAAADrwXDmUVXdO8mR09cXk7w5Sbr7R7dPNAAAAAAWbWvL1j6R5B+SPKG7L0qSqvql7ZIKAAAAgHVha8vWnpzk8iTvrapXVdXBSWor4wEAAADYwQzLo+5+a3cfkeS+Sd6b5MVJ7lxVf1pVP769AgIAAACwOKvZMPua7n5jdz8xyd5JPpzZT2ADAAAAYAe3zfJoqe6+srtP6O6D5xUIAAAAgPVjTeURAAAAADsX5REAAAAAQ3Mtj6rqkKq6sKouqqqXbmXcU6qqq+rAeeYBAAAAYG3mVh5V1S5Jjk9yaJL9kxxZVfuvMO52SV6U5Ix5ZQEAAADg5pnnzKODklzU3Z/q7uuSnJjk8BXG/T9JfjvJ1+eYBQAAAICbYZ7l0V5JLllyfOl07luq6sFJ9unuv9naharqqKo6q6rOuuKKK275pAAAAACsaGEbZlfVrZL8QZKXbGtsd5/Q3Qd294GbN2+efzgAAAAAksy3PLosyT5Ljveezm1xuyQPSPL3VXVxkocmOdWm2QAAAADrxzzLozOT7FdV96yqXZMckeTULU9291e6e8/u3re7903yoSSHdfdZc8wEAAAAwBrMrTzq7huSHJ3knUkuSHJSd59XVcdV1WHzel0AAAAAbjmb5nnx7j4tyWnLzh0zGPvoeWYBAAAAYO0WtmE2AAAAAOuf8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwNNfyqKoOqaoLq+qiqnrpCs//16o6v6o+VlWnV9U95pkHAAAAgLWZW3lUVbskOT7JoUn2T3JkVe2/bNiHkxzY3T+Y5JQkvzOvPAAAAACs3TxnHh2U5KLu/lR3X5fkxCSHLx3Q3e/t7munww8l2XuOeQAAAABYo3mWR3sluWTJ8aXTuZHnJvnblZ6oqqOq6qyqOuuKK664BSMCAAAAsDXrYsPsqnpGkgOT/O5Kz3f3Cd19YHcfuHnz5u0bDgAAAGAntmmO174syT5Ljveezt1EVT02yW8keVR3f2OOeQAAAABYo3nOPDozyX5Vdc+q2jXJEUlOXTqgqh6U5M+SHNbdX5hjFgAAAABuhrmVR919Q5Kjk7wzyQVJTuru86rquKo6bBr2u0lum+TkqvpIVZ06uBwAAAAACzDPZWvp7tOSnLbs3DFLHj92nq8PAAAAwHdnXWyYDQAAAMD6pDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMCQ8ggAAACAIeURAAAAAEPKIwAAAACGlEcAAAAADM21PKqqQ6rqwqq6qKpeusLz31NVb56eP6Oq9p1nHgAAAADWZm7lUVXtkuT4JIcm2T/JkVW1/7Jhz01yZXffK8nLk/z2vPIAAAAAsHbznHl0UJKLuvtT3X1dkhOTHL5szOFJXjs9PiXJwVVVc8wEAAAAwBpUd8/nwlVPTXJId//n6fiZSX64u49eMubcacyl0/EnpzFfXHato5IcNR3eJ8mFcwm9PuyZ5IvbHMV65N5tbO7fxub+bVzu3cbm/m1c7t3G5v5tbO7fxrWj37t7dPfmlZ7YtL2T3BzdfUKSExadY3uoqrO6+8BF52Dt3LuNzf3b2Ny/jcu929jcv43LvdvY3L+Nzf3buHbmezfPZWuXJdlnyfHe07kVx1TVpiS3T/KlOWYCAAAAYA3mWR6dmWS/qrpnVe2a5Igkpy4bc2qSZ0+Pn5rkPT2vdXQAAAAArNnclq119w1VdXSSdybZJcmru/u8qjouyVndfWqSP0/yuqq6KMmXMyuYdnY7xfK8HZR7t7G5fxub+7dxuXcbm/u3cbl3G5v7t7G5fxvXTnvv5rZhNgAAAAAb3zyXrQEAAACwwSmPAAAAABhSHq0TVfXqqvpCVZ276CysTVXtU1Xvrarzq+q8qnrRojOxelW1W1X9c1V9dLp/L1t0Jtamqnapqg9X1dsXnYW1qaqLq+rjVfWRqjpr0XlYvaq6Q1WdUlWfqKoLquphi87E6lTVfaY/c1u+rq6qFy86F6tXVb80/Z3l3Kp6U1XttuhMrE5VvWi6b+f5c7f+rfRv9Kq6U1W9u6r+dfr1jovMuD0pj9aP1yQ5ZNEhuFluSPKS7t4/yUOT/EJV7b/gTKzeN5I8prsfmOSAJIdU1UMXnIm1eVGSCxYdgpvtR7v7gO4+cNFBWJNXJHlHd983yQPjz+CG0d0XTn/mDoP8zyQAAAWrSURBVEjykCTXJnnLgmOxSlW1V5JfTHJgdz8gsx9M5IcObQBV9YAkz0tyUGb/3XxCVd1rsanYhtfkO/+N/tIkp3f3fklOn453CsqjdaK735/ZT5xjg+nuy7v7nOnxVzP7C/Rei03FavXM16bDW09ffpLABlFVeyd5fJL/vegssLOoqtsneWRmPzU33X1dd1+12FTcTAcn+WR3f3rRQViTTUl2r6pNSfZI8tkF52F17pfkjO6+trtvSPK+JE9ecCa2YvBv9MOTvHZ6/NokP7ldQy2Q8ghuQVW1b5IHJTljsUlYi2nZ00eSfCHJu7vb/ds4/jDJryT55qKDcLN0kndV1dlVddSiw7Bq90xyRZL/My0Z/d9VdZtFh+JmOSLJmxYdgtXr7suS/F6SzyS5PMlXuvtdi03FKp2b5D9V1fdV1R5JHpdknwVnYu3u0t2XT48/l+QuiwyzPSmP4BZSVbdN8pdJXtzdVy86D6vX3TdO0/f3TnLQNK2Yda6qnpDkC9199qKzcLM9orsfnOTQzJb8PnLRgViVTUkenORPu/tBSa7JTjRtf0dRVbsmOSzJyYvOwupN+6scnlmJe7ckt6mqZyw2FavR3Rck+e0k70ryjiQfSXLjQkPxXenuzk60YkF5BLeAqrp1ZsXRG7r7rxadh5tnWnbx3th/bKN4eJLDquriJCcmeUxVvX6xkViL6f+gp7u/kNmeKwctNhGrdGmSS5fM0jwlszKJjeXQJOd09+cXHYQ1eWySf+vuK7r7+iR/leRHFpyJVeruP+/uh3T3I5NcmeRfFp2JNft8Vd01SaZfv7DgPNuN8gi+S1VVme37cEF3/8Gi87A2VbW5qu4wPd49yY8l+cRiU7Ea3f1r3b13d++b2dKL93S3//u6QVTVbarqdlseJ/nxzKb0s8519+eSXFJV95lOHZzk/AVG4uY5MpasbUSfSfLQqtpj+jvowbFh/YZRVXeefr17ZvsdvXGxibgZTk3y7Onxs5P89QKzbFebFh2Amap6U5JHJ9mzqi5N8j+7+88Xm4pVeniSZyb5+LRvTpL8eneftsBMrN5dk7y2qnbJrFA/qbv9yHeYv7skecvs3z7ZlOSN3f2OxUZiDV6Y5A3T0qdPJfnZBedhDabC9seSPH/RWVib7j6jqk5Jck5mP/H3w0lOWGwq1uAvq+r7klyf5Bf8sIH1baV/oyf5rSQnVdVzk3w6ydMWl3D7qtkyPQAAAAD4TpatAQAAADCkPAIAAABgSHkEAAAAwJDyCAAAAIAh5REAAAAAQ8ojAIBtqKq7VNUbq+pTVXV2VX2wqp606FwAANuD8ggAYCuqqpK8Ncn7u/sHuvshSY5IsveycZsWkQ8AYN6quxedAQBg3aqqg5Mc092PWuG55yR5cpLbJtklyZOSvDrJDyS5NslR3f2xqjo2yde6+/em7zs3yROmy7wjydlJHpzkvCTP6u5r5/meAADWwswjAICtu3+Sc7by/IOTPHUql16W5MPd/YNJfj3JX6zi+vdJ8ifdfb8kVyf5+e8yLwDALUp5BACwBlV1fFV9tKrOnE69u7u/PD1+RJLXJUl3vyfJ91XV927jkpd09wemx6+frgEAsG4ojwAAtu68zGYXJUm6+xeSHJxk83TqmlVc44bc9O9duy15vHwPAXsKAADrivIIAGDr3pNkt6r6L0vO7TEY+w9JfiZJqurRSb7Y3VcnuThTAVVVD05yzyXfc/eqetj0+KeT/OMtlhwA4BZgw2wAgG2oqrsmeXmSH05yRWazjV6ZZPckB3b30dO4O2XlDbN3T/LXSfZKckaShyU5dLr8O5KcleQhSc5P8kwbZgMA64nyCABgQapq3yRv7+4HLDgKAMCQZWsAAAAADJl5BAAAAMCQmUcAAAAADCmPAAAAABhSHgEAAAAwpDwCAAAAYEh5BAAAAMDQ/wUyf0osJGeIiQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHBWaLNXGeI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "aed68e56-9dcc-44bc-e9f3-2eab0f85e0b1"
      },
      "source": [
        "\"\"\"num_classes_seen = 100\n",
        "dif_accuracies=printAccuracyDifference(net,old_accuracies, num_classes_seen)\n",
        "dif_accuracies\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 0.784, 0.0),\n",
              " (2, 0.902, 0.0),\n",
              " (3, 0.865, 0.0),\n",
              " (4, 0.904, 0.0),\n",
              " (5, 0.843, 0.0),\n",
              " (6, 0.891, 0.0),\n",
              " (7, 0.89, 0.0),\n",
              " (8, 0.918, 0.0),\n",
              " (9, 0.884, 0.0),\n",
              " (10, 0.888, 0.888)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjGRIqDtNQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}